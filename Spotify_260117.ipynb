{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import uuid\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12440619'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '02'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12434660'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12440619'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4080a558",
   "metadata": {},
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    558\u001b[39m args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    491\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mEndPointInternalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     10\u001b[39m registration_triples_b = [\n\u001b[32m     11\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type foaf:Person .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Agent .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <http://vivoweb.org/ontology/core#identifier> \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m12440619\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^^xsd:string .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m ]\n\u001b[32m     20\u001b[39m role_triples = [\n\u001b[32m     21\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_writer_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     23\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregistration_triples_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m engine.insert(registration_triples_b, prefixes=prefixes)\n\u001b[32m     28\u001b[39m engine.insert(role_triples, prefixes=prefixes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/starvers/starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:938\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m URITooLong(e.read())\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m e.code == \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mEndPointInternalError\u001b[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "registration_triples_a = [\n",
    "    f':{student_a} rdf:type foaf:Person .',\n",
    "    f':{student_a} rdf:type prov:Agent .',\n",
    "    f':{student_a} foaf:givenName \"Miha\" .',\n",
    "    f':{student_a} foaf:familyName \"Prah\" .',\n",
    "    f':{student_a} <http://vivoweb.org/ontology/core#identifier> \"12434660\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "registration_triples_b = [\n",
    "    f':{student_b} rdf:type foaf:Person .',\n",
    "    f':{student_b} rdf:type prov:Agent .',\n",
    "    f':{student_b} foaf:givenName \"Jakov\" .',\n",
    "    f':{student_b} foaf:familyName \"Mutvar\" .',\n",
    "    f':{student_b} <http://vivoweb.org/ontology/core#identifier> \"12440619\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(registration_triples_a, prefixes=prefixes)\n",
    "engine.insert(registration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The dataset used in this project is the Spotify 1 Million Tracks collection obtained from Kaggle, containing roughly\n",
    "one million songs with detailed metadata—such as artist name, track title, release year, genre, and an engagement-based\n",
    "popularity score—alongside Spotify’s engineered audio features, including danceability, energy, loudness, acousticness, instrumentalness,\n",
    "valence, tempo, and duration. These attributes describe the intrinsic characteristics of each track independently of user behaviour and\n",
    "therefore allow the construction of models that attempt to estimate popularity solely from acoustic and contextual properties.\n",
    "The business scenario motivating this analysis is that of a music-streaming platform seeking to evaluate newly ingested tracks before\n",
    "any substantial listening history exists: because early playlist placement and promotion strongly influence long-term performance, the\n",
    "platform requires a data-driven mechanism to identify promising tracks based only on their audio profile and metadata. The dataset aligns\n",
    "directly with this need, providing a large, diverse basis for analysing whether a track’s inherent musical properties can serve as reliable\n",
    "predictors of its eventual popularity.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The primary objective of the streaming platform in this scenario is to strengthen its early decision-making for newly released\n",
    "or newly ingested tracks, for which no meaningful user engagement data is yet available. By predicting the likelihood that a track\n",
    "will achieve above-average popularity using only its intrinsic acoustic and metadata attributes, the platform aims to improve the\n",
    "efficiency of its playlist-curation and recommendation processes. More accurate early assessments enable the platform to allocate promotional\n",
    "exposure more selectively, reduce dependence on manual curation, and increase listener engagement by prioritizing content with high\n",
    "potential impact. Ultimately, the objective is to support more effective catalogue management in a context where the volume of incoming\n",
    "tracks exceeds the platform’s capacity for human evaluation.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "Business success in this context is defined by measurable improvements in how the platform identifies and promotes promising tracks\n",
    "before user engagement signals accumulate. Success would be reflected in higher downstream listener engagement for tracks selected through\n",
    "the predictive system compared with those promoted under existing heuristics, as well as reductions in manual curation effort due to increased\n",
    "automation of early-stage selection. Additionally, successful deployment would lead to more efficient allocation of promotional resources,\n",
    "observable through improved performance of curated playlists or early-exposure campaigns. These outcomes must be attributable to the predictive\n",
    "system’s ability to surface high-potential tracks earlier and more consistently than current operational processes.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The central data mining goal is to construct and evaluate a predictive model that estimates a track’s future popularity class\n",
    "based solely on the attributes available at the time of ingestion, namely its audio features and metadata. This involves identifying\n",
    "which features contribute most strongly to popularity outcomes, determining whether popularity can be reliably inferred from a track’s\n",
    "intrinsic characteristics, and quantifying the model’s ability to generalize across diverse genres and time periods. Beyond predictive\n",
    "accuracy, the analysis also seeks to generate interpretable insights into the relationship between musical properties and commercial performance.\n",
    "The overarching goal is to determine whether such a model can meaningfully support the platform’s early-stage decision-making process.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "The success of the data mining effort is assessed through model-based performance metrics that quantify how reliably popularity can\n",
    "be predicted from the available features. Suitable criteria include achieving a classification performance that clearly exceeds a trivial\n",
    "or random baseline, maintaining stable results across validation folds, and demonstrating adequate sensitivity to tracks in the higher-popularity\n",
    "classes, as these are the cases of greatest business interest. The model should show consistent behaviour across genres and release years, indicating\n",
    "that predictive patterns are not confined to narrow subsets of the data. In addition, the resulting feature-importance patterns or model explanations\n",
    "should be coherent with domain understanding and provide actionable insights into the drivers of popularity.\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "Several AI-related risks must be considered in this scenario. Because popularity is strongly influenced by prior exposure and historical\n",
    "preference patterns, a model trained on such data may inadvertently reinforce existing biases—for example, favouring established artists\n",
    "or mainstream genres while disadvantaging niche or underrepresented categories. The dataset lacks demographic or contextual interaction\n",
    "data, making it difficult to detect or mitigate such systemic effects. There is also a risk of temporal drift, as musical tastes and platform\n",
    "dynamics evolve, potentially degrading model performance over time if not monitored. Finally, deploying a popularity-prediction model introduces\n",
    "the possibility of creating self-fulfilling feedback loops, where the system boosts tracks it predicts to be successful, thereby influencing\n",
    "the very outcome it attempts to measure. These risks necessitate careful evaluation and ongoing monitoring before operational use.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7135ecd2-f53f-46c3-a728-51bf42f8c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_spotify_data_code_writer = student_a\n",
    "\n",
    "spotify_data_path = os.path.join(\"data\")\n",
    "#print(spotify_data_path )\n",
    "def load_spotify_data()-> pd.DataFrame:\n",
    "\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(spotify_data_path, 'spotify_data.csv')\n",
    "    raw_data = pd.read_csv(input_file,  sep=',', header = 0)\n",
    "    # sampled = raw_data.sample(n=2000, random_state=None)\n",
    "\n",
    "    # save_new_sample = False\n",
    "\n",
    "    # if(save_new_sample): \n",
    "    #     sampled.to_csv(\"data/sampled.csv\", index=False)\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_spotify_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed7ee4c7-37a7-4ad2-a0ba-4e394ba690fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    558\u001b[39m args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    491\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mEndPointInternalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      2\u001b[39m \u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix_header\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[33;43mSELECT * WHERE \u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[33;43m  ?s ?p ?o .\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[38;5;130;43;01m}}\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[33;43mLIMIT 5\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/starvers/starvers.py:413\u001b[39m, in \u001b[36mTripleStoreEngine.query\u001b[39m\u001b[34m(self, select_statement, timestamp, yn_timestamp_query, as_df)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m#self.sparql_get_with_post.queryType = 'SELECT'\u001b[39;00m\n\u001b[32m    412\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRetrieving results ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_get_with_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe result has the return type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult._get_responseFormat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m as_df:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:938\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m URITooLong(e.read())\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m e.code == \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mEndPointInternalError\u001b[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "source": [
    "engine.query(f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT * WHERE {{\n",
    "  ?s ?p ?o .\n",
    "}}\n",
    "LIMIT 5\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d12fc1-a23b-45fc-9507-dc77cc5d79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"b8bac193-c4e6-4e31-9134-b23e001e279c\" # Generate once\n",
    "\n",
    "load_spotify_data_executor = [\n",
    "    f':load_spotify_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(load_spotify_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"c600e15c-87a9-4e2a-be85-b6c2a3014210\" # Generate once\n",
    "\n",
    "ld_report = \"\"\"The Spotify track dataset was loaded from its original CSV source to enable an initial inspection of its structure and\n",
    "content. At this stage, the focus is on understanding what information is available in the dataset, including the range\n",
    "of metadata fields and audio features provided by Spotify, as well as their basic data types and semantics. For practical\n",
    "reasons, a reduced sample of the full dataset may be used during exploration to ensure manageable computation times.\n",
    "\"\"\"\n",
    "\n",
    "load_spotify_data_activity = [\n",
    "    ':load_spotify_data rdf:type prov:Activity .',\n",
    "    ':load_spotify_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_spotify_data rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':load_spotify_data rdfs:comment \"\"\"{ld_report}\"\"\" .', \n",
    "    f':load_spotify_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_spotify_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_spotify_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_spotify_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT of activity\n",
    "    ':load_spotify_data prov:used :raw_data .',\n",
    "    ':load_spotify_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_spotify_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_spotify_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further descibe the raw data using Croissant\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \\'Spotify 1 Million Tracks Dataset\\' .',\n",
    "    ':raw_data sc:description \\'Dataset containing metadata and audio features for approximately one million Spotify tracks.\\' .',\n",
    "\n",
    "    # File object describing the CSV source\n",
    "    ':spotify_csv rdf:type cr:FileObject .',\n",
    "    ':spotify_csv sc:name \\'spotify_tracks.csv\\' .',\n",
    "    ':spotify_csv sc:encodingFormat \\'text/csv\\' .',\n",
    "    ':raw_data sc:distribution :spotify_csv .',\n",
    "\n",
    "    # RecordSet containing the table\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \\'Spotify tracks recordset\\' .',\n",
    "    ':raw_recordset cr:source :spotify_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "\n",
    "    # ---------------------- FIELD DEFINITIONS ----------------------\n",
    "\n",
    "    # ID field\n",
    "    ':raw_recordset cr:field :field_id .',\n",
    "    ':field_id rdf:type cr:Field .',\n",
    "    ':field_id sc:name \\'ID\\' .',\n",
    "    ':field_id sc:description \\'Unique index of the track in the dataset\\' .',\n",
    "    ':field_id cr:dataType xsd:integer .',\n",
    "    \n",
    "    # Artist name\n",
    "    ':raw_recordset cr:field :field_artist .',\n",
    "    ':field_artist rdf:type cr:Field .',\n",
    "    ':field_artist sc:name \\'artist_name\\' .',\n",
    "    ':field_artist sc:description \\'Name of the artist associated with the track\\' .',\n",
    "    ':field_artist cr:dataType xsd:string .',\n",
    "    \n",
    "    # Track name\n",
    "    ':raw_recordset cr:field :field_track_name .',\n",
    "    ':field_track_name rdf:type cr:Field .',\n",
    "    ':field_track_name sc:name \\'track_name\\' .',\n",
    "    ':field_track_name sc:description \\'Title of the track\\' .',\n",
    "    ':field_track_name cr:dataType xsd:string .',\n",
    "    \n",
    "    # Track ID\n",
    "    ':raw_recordset cr:field :field_track_id .',\n",
    "    ':field_track_id rdf:type cr:Field .',\n",
    "    ':field_track_id sc:name \\'track_id\\' .',\n",
    "    ':field_track_id sc:description \\'Spotify identifier of the track\\' .',\n",
    "    ':field_track_id cr:dataType xsd:string .',\n",
    "    \n",
    "    # Popularity\n",
    "    ':raw_recordset cr:field :field_popularity .',\n",
    "    ':field_popularity rdf:type cr:Field .',\n",
    "    ':field_popularity sc:name \\'popularity\\' .',\n",
    "    ':field_popularity sc:description \\'Spotify popularity score from 0 to 100\\' .',\n",
    "    ':field_popularity cr:dataType xsd:integer .',\n",
    "    \n",
    "    # Year\n",
    "    ':raw_recordset cr:field :field_year .',\n",
    "    ':field_year rdf:type cr:Field .',\n",
    "    ':field_year sc:name \\'year\\' .',\n",
    "    ':field_year sc:description \\'Release year of the track\\' .',\n",
    "    ':field_year cr:dataType xsd:gYear .',\n",
    "    \n",
    "    # Genre\n",
    "    ':raw_recordset cr:field :field_genre .',\n",
    "    ':field_genre rdf:type cr:Field .',\n",
    "    ':field_genre sc:name \\'genre\\' .',\n",
    "    ':field_genre sc:description \\'Genre label assigned to the track\\' .',\n",
    "    ':field_genre cr:dataType xsd:string .',\n",
    "    \n",
    "    # Danceability\n",
    "    ':raw_recordset cr:field :field_danceability .',\n",
    "    ':field_danceability rdf:type cr:Field .',\n",
    "    ':field_danceability sc:name \\'danceability\\' .',\n",
    "    ':field_danceability sc:description \\'Suitability of a track for dancing\\' .',\n",
    "    ':field_danceability cr:dataType xsd:double .',\n",
    "    \n",
    "    # Energy\n",
    "    ':raw_recordset cr:field :field_energy .',\n",
    "    ':field_energy rdf:type cr:Field .',\n",
    "    ':field_energy sc:name \\'energy\\' .',\n",
    "    ':field_energy sc:description \\'Perceptual measure of intensity and activity (0.0 to 1.0)\\' .',\n",
    "    ':field_energy cr:dataType xsd:double .',\n",
    "    \n",
    "    # Key\n",
    "    ':raw_recordset cr:field :field_key .',\n",
    "    ':field_key rdf:type cr:Field .',\n",
    "    ':field_key sc:name \\'key\\' .',\n",
    "    ':field_key sc:description \\'Estimated musical key of the track, encoded as integers 0–11\\' .',\n",
    "    ':field_key cr:dataType xsd:integer .',\n",
    "    \n",
    "    # Loudness\n",
    "    ':raw_recordset cr:field :field_loudness .',\n",
    "    ':field_loudness rdf:type cr:Field .',\n",
    "    ':field_loudness sc:name \\'loudness\\' .',\n",
    "    ':field_loudness sc:description \\'Overall loudness of the track in decibels (approx. -60 to 0)\\' .',\n",
    "    ':field_loudness cr:dataType xsd:double .',\n",
    "    \n",
    "    # Mode\n",
    "    ':raw_recordset cr:field :field_mode .',\n",
    "    ':field_mode rdf:type cr:Field .',\n",
    "    ':field_mode sc:name \\'mode\\' .',\n",
    "    ':field_mode sc:description \\'Modality of the track: Major (1) or Minor (0)\\' .',\n",
    "    ':field_mode cr:dataType xsd:integer .',\n",
    "    \n",
    "    # Speechiness\n",
    "    ':raw_recordset cr:field :field_speechiness .',\n",
    "    ':field_speechiness rdf:type cr:Field .',\n",
    "    ':field_speechiness sc:name \\'speechiness\\' .',\n",
    "    ':field_speechiness sc:description \\'Presence of spoken words in the track\\' .',\n",
    "    ':field_speechiness cr:dataType xsd:double .',\n",
    "    \n",
    "    # Acousticness\n",
    "    ':raw_recordset cr:field :field_acousticness .',\n",
    "    ':field_acousticness rdf:type cr:Field .',\n",
    "    ':field_acousticness sc:name \\'acousticness\\' .',\n",
    "    ':field_acousticness sc:description \\'Confidence measure of whether the track is acoustic (0.0 to 1.0)\\' .',\n",
    "    ':field_acousticness cr:dataType xsd:double .',\n",
    "    \n",
    "    # Instrumentalness\n",
    "    ':raw_recordset cr:field :field_instrumentalness .',\n",
    "    ':field_instrumentalness rdf:type cr:Field .',\n",
    "    ':field_instrumentalness sc:name \\'instrumentalness\\' .',\n",
    "    ':field_instrumentalness sc:description \\'Likelihood the track contains no vocals (0.0 to 1.0)\\' .',\n",
    "    ':field_instrumentalness cr:dataType xsd:double .',\n",
    "    \n",
    "    # Liveness\n",
    "    ':raw_recordset cr:field :field_liveness .',\n",
    "    ':field_liveness rdf:type cr:Field .',\n",
    "    ':field_liveness sc:name \\'liveness\\' .',\n",
    "    ':field_liveness sc:description \\'Probability the track was recorded live\\' .',\n",
    "    ':field_liveness cr:dataType xsd:double .',\n",
    "    \n",
    "    # Valence\n",
    "    ':raw_recordset cr:field :field_valence .',\n",
    "    ':field_valence rdf:type cr:Field .',\n",
    "    ':field_valence sc:name \\'valence\\' .',\n",
    "    ':field_valence sc:description \\'Musical positiveness conveyed by the track\\' .',\n",
    "    ':field_valence cr:dataType xsd:double .',\n",
    "    \n",
    "    # Tempo\n",
    "    ':raw_recordset cr:field :field_tempo .',\n",
    "    ':field_tempo rdf:type cr:Field .',\n",
    "    ':field_tempo sc:name \\'tempo\\' .',\n",
    "    ':field_tempo sc:description \\'Tempo of the track in beats per minute (BPM)\\' .',\n",
    "    ':field_tempo cr:dataType xsd:double .',\n",
    "    \n",
    "    # Duration\n",
    "    ':raw_recordset cr:field :field_duration .',\n",
    "    ':field_duration rdf:type cr:Field .',\n",
    "    ':field_duration sc:name \\'duration_ms\\' .',\n",
    "    ':field_duration sc:description \\'Track duration in milliseconds\\' .',\n",
    "    ':field_duration cr:dataType xsd:integer .',\n",
    "    \n",
    "    # Time signature\n",
    "    ':raw_recordset cr:field :field_time_signature .',\n",
    "    ':field_time_signature rdf:type cr:Field .',\n",
    "    ':field_time_signature sc:name \\'time_signature\\' .',\n",
    "    ':field_time_signature sc:description \\'Estimated time signature indicating the number of beats per bar (e.g., 3 to 7)\\' .',\n",
    "    ':field_time_signature cr:dataType xsd:integer .',\n",
    "\n",
    "]\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also the output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "\n",
    "    # Reuse all fields\n",
    "    ':recordset cr:field :field_id .',\n",
    "    ':recordset cr:field :field_artist .',\n",
    "    ':recordset cr:field :field_track_name .',\n",
    "    ':recordset cr:field :field_track_id .',\n",
    "    ':recordset cr:field :field_popularity .',\n",
    "    ':recordset cr:field :field_year .',\n",
    "    ':recordset cr:field :field_genre .',\n",
    "    ':recordset cr:field :field_danceability .',\n",
    "    ':recordset cr:field :field_energy .',\n",
    "    ':recordset cr:field :field_key .',\n",
    "    ':recordset cr:field :field_loudness .',\n",
    "    ':recordset cr:field :field_mode .',\n",
    "    ':recordset cr:field :field_speechiness .',\n",
    "    ':recordset cr:field :field_acousticness .',\n",
    "    ':recordset cr:field :field_instrumentalness .',\n",
    "    ':recordset cr:field :field_liveness .',\n",
    "    ':recordset cr:field :field_valence .',\n",
    "    ':recordset cr:field :field_tempo .',\n",
    "    ':recordset cr:field :field_duration .',\n",
    "    ':recordset cr:field :field_time_signature .',\n",
    "]\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to the fields\n",
    "units_triples = [\n",
    "    ':field_loudness qudt:unit siu:decibel .',\n",
    "    ':field_tempo qudt:unit qudt:BeatsPerMinute .',\n",
    "    ':field_duration qudt:unit siu:millisecond .',\n",
    "    ':field_danceability qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_energy qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_acousticness qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_instrumentalness qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_valence qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_liveness qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_speechiness qudt:unit qudt:DimensionlessUnit .',\n",
    "]\n",
    "engine.insert(units_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e9368-b49b-4eab-9605-bb0ef4b8d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in dataset:\")\n",
    "print(data.columns.tolist())\n",
    "\n",
    "print(\"\\nShape (rows, columns):\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa3375-9c16-4d1c-afec-85c79fe0b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData overview:\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03b7df-4c13-4c97-98e0-8d21786a5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values per column:\")\n",
    "print(data.isna().sum())\n",
    "\n",
    "print(\"\\nPercentage missing per column:\")\n",
    "print(data.isna().mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b75df-1523-4523-b8c3-c0a07aae0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNumber of duplicate rows:\")\n",
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d48ac5-826d-4e6a-a89e-dd1ee3ca0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_checks = {\n",
    "    'popularity': (0, 100),\n",
    "    'year': (2000, 2023),\n",
    "    'danceability': (0.0, 1.0),\n",
    "    'energy': (0.0, 1.0),\n",
    "    'speechiness': (0.0, 1.0),\n",
    "    'acousticness': (0.0, 1.0),\n",
    "    'instrumentalness': (0.0, 1.0),\n",
    "    'liveness': (0.0, 1.0),\n",
    "    'valence': (0.0, 1.0),\n",
    "    'key': (-1, 11),\n",
    "    'mode': (0, 1),\n",
    "    'time_signature': (3, 7),\n",
    "    'loudness': (-60.0, 0.0),\n",
    "    'tempo': (0.0, 300.0),\n",
    "    'duration_ms': (0, None),\n",
    "}\n",
    "\n",
    "print(\"Invalid range value counts:\")\n",
    "\n",
    "for col, (low, high) in range_checks.items():\n",
    "    if col not in data.columns:\n",
    "        print(f\"{col}: column not found\")\n",
    "        continue\n",
    "\n",
    "    invalid_low = (data[col] < low).sum() if low is not None else 0\n",
    "    invalid_high = (data[col] > high).sum() if high is not None else 0\n",
    "\n",
    "    print(\n",
    "        f\"{col}: below {low} = {invalid_low}, \"\n",
    "        f\"above {high} = {invalid_high}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4753586-cb4b-4eee-9897-70391e9b1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_understanding_comment = \"\"\"\n",
    "The dataset contains 1,159,764 rows and 20 columns, indicating a large-scale collection of Spotify tracks suitable\n",
    "for robust statistical analysis and model training. No duplicate records were identified in the dataset. A small\n",
    "number of missing values were observed, specifically 15 missing entries in the artist_name column and 1 missing\n",
    "entry in the track_name column. Given the very low proportion of missing data relative to the dataset size, these\n",
    "records are not expected to bias the analysis and will be removed during the data preparation phase.\n",
    "\n",
    "In addition to missing-value checks, all numerical attributes were validated against their expected value ranges.\n",
    "Most features fully comply with their defined bounds. Two exceptions were identified: 13,888 tracks exhibit\n",
    "time_signature values below the expected minimum of 3, and 1,198 tracks have loudness values exceeding the\n",
    "expected upper bound of 0 dB. All other attributes, including popularity, year, tempo, and normalized audio\n",
    "features, show no out-of-range values. These deviations will be considered explicitly in subsequent data\n",
    "preparation steps.\n",
    "\"\"\"\n",
    "\n",
    "data_understanding_report = [\n",
    "    ':data_volume_report rdf:type prov:Entity .',\n",
    "    ':data_volume_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':data_volume_report rdfs:label \"Dataset size and basic data quality\" .',\n",
    "    f':data_volume_report rdfs:comment \"\"\"{data_understanding_comment}\"\"\" .',\n",
    "    ':data_volume_report prov:wasDerivedFrom :data .',\n",
    "]\n",
    "\n",
    "engine.insert(data_understanding_report, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1077f-c079-4ced-8fde-ffa4f9d4ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = data.loc[:, ~data.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "summary_data = summary_data.drop(columns=['year'], errors='ignore')\n",
    "\n",
    "summary_data = summary_data.select_dtypes(include='number')\n",
    "\n",
    "summary_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93243e-42ea-4984-a0d8-28e3f436b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_sk = now()\n",
    "\n",
    "numeric_cols = [\n",
    "    col for col in data.select_dtypes(include=[np.number]).columns\n",
    "    if not col.lower().startswith(\"unnamed\")\n",
    "]\n",
    "\n",
    "print(\"\\nSkewness per numeric column:\")\n",
    "print(data[numeric_cols].skew().sort_values(ascending=False))\n",
    "\n",
    "output_fig_dir = \"figures\"\n",
    "os.makedirs(output_fig_dir, exist_ok=True)\n",
    "\n",
    "fig_path_skew = os.path.join(\n",
    "    output_fig_dir,\n",
    "    f\"du_numeric_histograms_2026_01_15.png\"\n",
    ")\n",
    "\n",
    "data[numeric_cols].hist(bins=30, figsize=(15, 12))\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path_skew, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "end_time_sk = now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1dbab0-d51e-4a4d-b055-280d110ccc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a417b49-bead-4729-84de-6e5dce70761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_ass_uuid_writer = '3a64927d-3eb9-40a4-b37d-d6dcc68aaa47'\n",
    "\n",
    "skewness_analysis_comment = \"\"\"\n",
    "The skewness analysis indicates that a large number of numerical features in the dataset\n",
    "deviate from symmetric distributions. Several audio attributes, including duration_ms,\n",
    "speechiness, and liveness, exhibit strong positive skewness, meaning that most tracks\n",
    "have relatively low values while a small number of tracks show very high values.\n",
    "Other features, such as instrumentalness, acousticness, and popularity, display\n",
    "moderate skewness, reflecting long-tail effects that are typical for streaming platforms.\n",
    "In contrast, features such as tempo, valence, and danceability are more evenly distributed.\n",
    "Negative skewness observed for energy and loudness suggests a concentration of tracks\n",
    "at higher values, which is consistent with contemporary music production practices.\n",
    "Overall, the results show that assumptions of normality do not hold for many variables,\n",
    "highlighting the importance of considering distributional properties in later analysis steps.\n",
    "\"\"\"\n",
    "\n",
    "skewness_report = [\n",
    "    ':du_skewness_report rdf:type prov:Entity .',\n",
    "    ':du_skewness_report rdfs:label \"2c Skewness Analysis\" .',\n",
    "    f':du_skewness_report rdfs:comment \"\"\"{skewness_analysis_comment}\"\"\" .',\n",
    "    ':du_skewness_report prov:wasGeneratedBy :skewness_analysis .',\n",
    "]\n",
    "\n",
    "skewness_analysis_activity = [\n",
    "    ':skewness_analysis rdf:type prov:Activity .',\n",
    "    ':skewness_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    ':skewness_analysis rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':skewness_analysis rdfs:comment \"\"\"{skewness_analysis_comment}\"\"\" .',\n",
    "    f':skewness_analysis prov:startedAtTime \"{start_time_sk}\"^^xsd:dateTime .',\n",
    "    f':skewness_analysis prov:endedAtTime \"{end_time_sk}\"^^xsd:dateTime .',\n",
    "    f':skewness_analysis prov:qualifiedAssociation :{sk_ass_uuid_writer} .',\n",
    "    f':{sk_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{sk_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT\n",
    "    ':skewness_analysis prov:used :data .',\n",
    "]\n",
    "\n",
    "skewness_plot_triples = [\n",
    "    ':du_numeric_histogram_plot rdf:type prov:Entity .',\n",
    "    ':du_numeric_histogram_plot rdfs:label \"Numeric Feature Histograms (Skewness Analysis)\" .',\n",
    "    ':du_numeric_histogram_plot prov:wasGeneratedBy :skewness_analysis .',\n",
    "    f':du_numeric_histogram_plot prov:atLocation \"{fig_path_skew}\" .',\n",
    "]\n",
    "\n",
    "engine.insert(skewness_plot_triples, prefixes=prefixes)\n",
    "engine.insert(skewness_report, prefixes=prefixes)\n",
    "engine.insert(skewness_analysis_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb19209-18d0-48c5-9a66-70dbd568e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "output_fig_dir = \"figures\"\n",
    "os.makedirs(output_fig_dir, exist_ok=True)\n",
    "\n",
    "fig_path = os.path.join(\n",
    "    output_fig_dir,\n",
    "    f\"du_numeric_distributions_2026_01_15.png\"\n",
    ")\n",
    "\n",
    "cols = 4\n",
    "rows = math.ceil(len(numeric_cols) / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.kdeplot(data[col], fill=True, ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "for j in range(len(numeric_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "#############################################\n",
    "# Data Understanding – Numeric Distributions Plot\n",
    "#############################################\n",
    "\n",
    "du_plot_ass_uuid = \"a2d3c4e5-1111-2222-3333-444455556666\"\n",
    "\n",
    "du_numeric_plot_activity = [\n",
    "    # Activity\n",
    "    f':du_numeric_distribution_analysis rdf:type prov:Activity .',\n",
    "    f':du_numeric_distribution_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    f':du_numeric_distribution_analysis rdfs:label \"Numeric Feature Distribution Analysis\" .',\n",
    "    f':du_numeric_distribution_analysis rdfs:comment \"\"\"Kernel density plots were generated for all numeric Spotify audio features to analyze their empirical distributions. This visualization supports the assessment of skewness, modality, and potential outliers, and informed later decisions regarding outlier handling and feature preprocessing.\"\"\" .',\n",
    "    f':du_numeric_distribution_analysis prov:qualifiedAssociation :{du_plot_ass_uuid} .',\n",
    "\n",
    "    # Association\n",
    "    f':{du_plot_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{du_plot_ass_uuid} prov:agent :{student_a} .',\n",
    "    f':{du_plot_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Figure entity\n",
    "    f':du_numeric_distribution_plot rdf:type prov:Entity .',\n",
    "    f':du_numeric_distribution_plot rdfs:label \"Numeric Feature Distributions (KDE plots)\" .',\n",
    "    f':du_numeric_distribution_plot prov:wasGeneratedBy :du_numeric_distribution_analysis .',\n",
    "    f':du_numeric_distribution_plot prov:atLocation \"{fig_path}\" .',\n",
    "]\n",
    "\n",
    "engine.insert(du_numeric_plot_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d6acd-7053-4d95-8fc4-4015a94c91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols_filtered = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in ['duration_ms', 'year', 'mode', 'time_signature']\n",
    "]\n",
    "\n",
    "cols = 3\n",
    "rows = math.ceil(len(numeric_cols_filtered) / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 2.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols_filtered):\n",
    "    sns.boxplot(x=data[col], ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "for j in range(len(numeric_cols_filtered), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "\n",
    "# --- Save boxplot grid ---\n",
    "output_fig_dir = \"figures\"\n",
    "os.makedirs(output_fig_dir, exist_ok=True)\n",
    "\n",
    "fig_path_box = os.path.join(\n",
    "    output_fig_dir,\n",
    "    f\"du_numeric_boxplots_filtered_2026_01_15.png\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path_box, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "boxplot_ass_uuid = \"976a215f-aa75-4c0f-ba6e-f6a22d0181f2\"\n",
    "\n",
    "boxplot_activity = [\n",
    "    ':du_outlier_boxplot_analysis rdf:type prov:Activity .',\n",
    "    ':du_outlier_boxplot_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_outlier_boxplot_analysis rdfs:label \"Outlier Inspection (Boxplots)\" .',\n",
    "    f':du_outlier_boxplot_analysis prov:qualifiedAssociation :{boxplot_ass_uuid} .',\n",
    "\n",
    "    f':{boxplot_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{boxplot_ass_uuid} prov:agent :{student_a} .',\n",
    "    f':{boxplot_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ':du_numeric_boxplots_filtered_plot rdf:type prov:Entity .',\n",
    "    ':du_numeric_boxplots_filtered_plot rdfs:label \"Filtered Numeric Feature Boxplots (Outlier Inspection)\" .',\n",
    "    ':du_numeric_boxplots_filtered_plot prov:wasGeneratedBy :du_outlier_boxplot_analysis .',\n",
    "    f':du_numeric_boxplots_filtered_plot prov:atLocation \"{fig_path_box}\" .',\n",
    "]\n",
    "engine.insert(boxplot_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3fb2c-eda1-4b9c-b9e4-cf8e305791e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fig_dir = \"figures\"\n",
    "os.makedirs(output_fig_dir, exist_ok=True)\n",
    "\n",
    "fig_path_corr = os.path.join(\n",
    "    output_fig_dir,\n",
    "    f\"du_correlation_heatmap_2026_01_15.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(data[numeric_cols].corr(), annot=False)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path_corr, dpi=150)\n",
    "plt.show()\n",
    "\n",
    "heatmap_ass_uuid = \"f95da175-cb54-4e04-8f9d-962d9e9319d9\"\n",
    "\n",
    "heatmap_activity = [\n",
    "    ':du_correlation_analysis rdf:type prov:Activity .',\n",
    "    ':du_correlation_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    ':du_correlation_analysis rdfs:label \"Correlation Analysis (Numeric Features)\" .',\n",
    "    f':du_correlation_analysis prov:qualifiedAssociation :{heatmap_ass_uuid} .',\n",
    "\n",
    "    f':{heatmap_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{heatmap_ass_uuid} prov:agent :{student_a} .',\n",
    "    f':{heatmap_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ':du_correlation_heatmap_plot rdf:type prov:Entity .',\n",
    "    ':du_correlation_heatmap_plot rdfs:label \"Correlation Heatmap (Numeric Features)\" .',\n",
    "    ':du_correlation_heatmap_plot prov:wasGeneratedBy :du_correlation_analysis .',\n",
    "    f':du_correlation_heatmap_plot prov:atLocation \"{fig_path_corr}\" .',\n",
    "]\n",
    "engine.insert(heatmap_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9522a96-9800-4898-a6f8-45d400b2df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(\n",
    "    data: pd.DataFrame,\n",
    "    columns=(\n",
    "        'danceability', 'energy', 'speechiness', 'acousticness',\n",
    "        'instrumentalness', 'liveness', 'valence', 'tempo', 'loudness'\n",
    "    )\n",
    ") -> dict:\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    tmp = data.copy().reset_index(drop=True)\n",
    "\n",
    "    for col in columns:\n",
    "        values = tmp[col].astype(float)\n",
    "\n",
    "        Q1 = values.quantile(0.25)\n",
    "        Q3 = values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        if IQR == 0 or np.isnan(IQR):\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        mask = (values < lower_bound) | (values > upper_bound)\n",
    "        outlier_indices = values[mask].index\n",
    "\n",
    "        results[col] = [\n",
    "            {\n",
    "                'index': int(idx),\n",
    "                'value': float(values.loc[idx])\n",
    "            }\n",
    "            for idx in outlier_indices\n",
    "        ]\n",
    "\n",
    "    all_outlier_indices = {\n",
    "        entry['index']\n",
    "        for col in results\n",
    "        for entry in results[col]\n",
    "    }\n",
    "\n",
    "    print(f\"Total unique outlier IDs across ALL columns: {len(all_outlier_indices)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "outliers_report = check_outliers(data)\n",
    "\n",
    "for col, outliers in outliers_report.items():\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0580e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outlier_report_comment = \"\"\"\n",
    "Outliers were identified using an IQR-based approach to assess the presence of extreme\n",
    "values across continuous audio features. The analysis was intended to characterize\n",
    "distributional properties rather than to remove observations, as many extreme values\n",
    "represent valid but infrequent musical characteristics. Consequently, outliers were\n",
    "documented but not excluded at this stage.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "outlier_report_entity = [\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    ':outlier_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':outlier_report rdfs:label \"Outlier Analysis\" .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{outlier_report_comment}\"\"\" .',\n",
    "]\n",
    "\n",
    "engine.insert(outlier_report_entity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f9c1c-c75b-4883-abea-1ebdcd15ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data_clean = data.dropna().copy()\n",
    "\n",
    "    VALID_BOUNDS = {\n",
    "        'popularity': (0, 100),\n",
    "        'year': (2000, 2023),\n",
    "        'danceability': (0.0, 1.0),\n",
    "        'energy': (0.0, 1.0),\n",
    "        'speechiness': (0.0, 1.0),\n",
    "        'acousticness': (0.0, 1.0),\n",
    "        'instrumentalness': (0.0, 1.0),\n",
    "        'liveness': (0.0, 1.0),\n",
    "        'valence': (0.0, 1.0),\n",
    "        'key': (-1, 11),\n",
    "        'mode': (0, 1),\n",
    "        'time_signature': (3, 7),\n",
    "        'loudness': (-60.0, 0.0),\n",
    "        'tempo': (0.0, 300.0),\n",
    "        'duration_ms': (0, None),\n",
    "    }\n",
    "\n",
    "    for col, (low, high) in VALID_BOUNDS.items():\n",
    "        if col not in data_clean.columns:\n",
    "            continue\n",
    "        if low is not None:\n",
    "            data_clean = data_clean[data_clean[col] >= low]\n",
    "        if high is not None:\n",
    "            data_clean = data_clean[data_clean[col] <= high]\n",
    "\n",
    "    def popularity_bucket(p: int) -> str:\n",
    "        if 0 <= p <= 20:\n",
    "            return \"Very bad\"\n",
    "        elif 21 <= p <= 40:\n",
    "            return \"Bad\"\n",
    "        elif 41 <= p <= 60:\n",
    "            return \"Average\"\n",
    "        elif 61 <= p <= 80:\n",
    "            return \"Good\"\n",
    "        elif 81 <= p <= 100:\n",
    "            return \"Very good\"\n",
    "        else:\n",
    "            return \"Invalid\"\n",
    "\n",
    "    data_clean[\"popularity_class\"] = data_clean[\"popularity\"].apply(popularity_bucket)\n",
    "\n",
    "    return data_clean.reset_index(drop=True)\n",
    "\n",
    "def create_data_preset(clean_data: pd.DataFrame, sample_size: int, random_state: int):\n",
    "    output_dir = \"data\"\n",
    "    output_filename = f\"sample_{sample_size}_10_01_2026.csv\"\n",
    "\n",
    "    data_preset = clean_data.sample(n=sample_size, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    data_preset.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Sample file name: {output_filename}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9a19c-5f46-4a20-9a87-78a390d408f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b298e01-2920-49d0-80ad-1ca6b56deef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ass_uuid_writer = '67c8989e-83b9-47ce-b517-de12d53af2f4'\n",
    "\n",
    "start_time_dp = now()\n",
    "data_clean = prepare_data(data)\n",
    "create_new_preset = False\n",
    "if (create_new_preset):\n",
    "    create_data_preset(data_clean, 100000, 42)\n",
    "end_time_dp = now()\n",
    "\n",
    "\n",
    "data_preparation_comment = \"\"\"\n",
    "The data preparation step was performed as a single processing activity.\n",
    "First, all rows containing missing values were removed. The number of such rows\n",
    "was very small compared to the overall dataset size, and the affected attributes\n",
    "are required for later analysis, making removal preferable to imputation.\n",
    "\n",
    "Second, records containing values outside predefined valid bounds were excluded\n",
    "to ensure semantic correctness and consistency of the data. These bounds were\n",
    "derived from domain knowledge of Spotify audio features and metadata.\n",
    "\n",
    "Identified outliers were not removed at this stage. The outlier analysis showed\n",
    "that extreme values mostly occur in features with naturally skewed or long-tailed\n",
    "distributions and likely represent valid but infrequent musical characteristics\n",
    "rather than data errors. Removing them globally could distort the underlying data\n",
    "distribution and reduce representativeness.\n",
    "\n",
    "Finally, a new categorical attribute was created by discretizing the popularity\n",
    "score into five ordered classes (Very bad, Bad, Average, Good, Very good). This\n",
    "derived attribute supports downstream classification and interpretation while\n",
    "preserving the original numerical popularity measure.\n",
    "\n",
    "As a final step of the data preparation process, a reproducible random subset of \n",
    "the prepared dataset was created for subsequent modeling experiments. From the fully \n",
    "cleaned and enriched data, a uniform random sample of 100,000 records was drawn using a \n",
    "fixed random seed to ensure reproducibility. This subset was persisted as a separate \n",
    "dataset artifact and is used consistently throughout the modeling phase in order to \n",
    "reduce computational effort while maintaining a representative distribution of the \n",
    "underlying data.\n",
    "\n",
    "Several additional preprocessing steps were considered but not applied. Feature\n",
    "scaling and normalization were not used because the chosen model type (Random\n",
    "Forest) does not depend on feature scaling. Techniques such as oversampling or\n",
    "undersampling to address class imbalance were also considered, but instead class\n",
    "weights were handled directly during model training.\n",
    "\n",
    "The creation of further derived attributes was also considered. Possible options\n",
    "included combining existing audio features or creating summary indicators.\n",
    "However, these were not applied because the model can already learn complex\n",
    "relationships between features, and the expected benefit was limited.\n",
    "\n",
    "Finally, potential external data sources were considered at a conceptual level.\n",
    "Examples include artist popularity, early streaming counts, or social media\n",
    "signals. These sources were not included in this analysis in order to keep the\n",
    "focus on audio features available at the time of track ingestion and to limit\n",
    "the scope of the project.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_preparation_activity = [\n",
    "    ':data_preparation rdf:type prov:Activity .',\n",
    "    ':data_preparation sc:isPartOf :data_preparation_phase .',\n",
    "    ':data_preparation rdfs:comment \"Data Preparation\" .',\n",
    "    f':data_preparation rdfs:comment \"\"\"{data_preparation_comment}\"\"\" .',\n",
    "    f':data_preparation prov:startedAtTime \"{start_time_dp}\"^^xsd:dateTime .',\n",
    "    f':data_preparation prov:endedAtTime \"{end_time_dp}\"^^xsd:dateTime .',\n",
    "    f':data_preparation prov:qualifiedAssociation :{dp_ass_uuid_writer} .',\n",
    "    f':{dp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dp_ass_uuid_writer} prov:agent :{student_a} .',\n",
    "    f':{dp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':data_preparation prov:used :data .',\n",
    "    ':data_clean rdf:type prov:Entity .',\n",
    "    ':data_clean prov:wasGeneratedBy :data_preparation .',\n",
    "    ':data_clean prov:wasDerivedFrom :data .',\n",
    "\n",
    "    ':data_preset rdf:type prov:Entity .',\n",
    "    ':data_preset rdf:type sc:Dataset .',\n",
    "    ':data_preset prov:wasGeneratedBy :data_preparation .',\n",
    "    ':data_preset prov:wasDerivedFrom :data_clean .',\n",
    "\n",
    "    ':data_preset_csv rdf:type prov:Entity .',\n",
    "    ':data_preset_csv rdf:type sc:File .',\n",
    "    ':data_preset_csv prov:wasGeneratedBy :data_preparation .',\n",
    "    ':data_preset_csv prov:atLocation \"data/\" .',\n",
    "    ':data_preset_csv prov:wasDerivedFrom :data_preset .',\n",
    "\n",
    "]\n",
    "\n",
    "engine.insert(data_preparation_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data_triples = [\n",
    "    ':data_clean rdf:type prov:Entity .',\n",
    "    ':data_clean rdf:type sc:Dataset .',\n",
    "    ':data_clean prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling_phase rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "The modeling task is formulated as a supervised multi-class classification problem, where the goal\n",
    "is to predict an ordered popularity class (Very bad, Bad, Average, Good, Very good) based on\n",
    "Spotify audio features and related metadata.\n",
    "\n",
    "Several candidate classification algorithms were considered. Multinomial logistic regression was\n",
    "identified as a simple and interpretable baseline, but its assumption of linear decision boundaries\n",
    "limits its ability to capture complex relationships between audio characteristics. Support Vector\n",
    "Machines were also considered due to their strong theoretical properties, however they require\n",
    "careful feature scaling and can be computationally expensive for larger datasets. Decision Trees\n",
    "offer non-linear modeling capabilities and interpretability, but are known to be prone to overfitting\n",
    "when used as single estimators.\n",
    "\n",
    "The Random Forest classifier was selected as the primary algorithm for the experiments. As an\n",
    "ensemble method that combines multiple decision trees trained on bootstrap samples with randomized\n",
    "feature selection, Random Forests effectively reduce variance while preserving the ability to model\n",
    "non-linear relationships and interactions between features. This is particularly suitable for the\n",
    "Spotify dataset, where popularity is influenced by complex combinations of audio attributes such as\n",
    "energy, danceability, tempo, and acousticness.\n",
    "\n",
    "In addition, Random Forests are robust to noise and outliers, require minimal assumptions about the\n",
    "underlying data distribution, and perform well with heterogeneous feature types. These properties\n",
    "make the algorithm well suited for exploratory and predictive modeling in a Business Intelligence\n",
    "context, where robustness, stability, and reproducibility are prioritized over highly specialized\n",
    "model tuning.\n",
    "\n",
    "Based on these considerations, the Random Forest classifier was chosen as the most suitable data\n",
    "mining algorithm for subsequent modeling and evaluation.\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    # Activity\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"Algorithm Selection and Evaluation Measures Definition\" .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "\n",
    "    # Association\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # ----------------------------\n",
    "    # Algorithm definition\n",
    "    # ----------------------------\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # ----------------------------\n",
    "    # Implementation definition (IMPORTANT: keep name consistent everywhere)\n",
    "    # ----------------------------\n",
    "    f':random_forest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    # ----------------------------\n",
    "    # Evaluation measures (classification)\n",
    "    # ----------------------------\n",
    "    f':accuracy_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':accuracy_measure rdfs:label \"Accuracy\" .',\n",
    "    f':accuracy_measure rdfs:comment \"Fraction of correctly classified instances across all classes.\" .',\n",
    "    f':accuracy_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    f':macro_f1_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':macro_f1_measure rdfs:label \"Macro-averaged F1-score\" .',\n",
    "    f':macro_f1_measure rdfs:comment \"F1-score computed per class and averaged equally; robust under class imbalance.\" .',\n",
    "    f':macro_f1_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    f':weighted_f1_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':weighted_f1_measure rdfs:label \"Weighted F1-score\" .',\n",
    "    f':weighted_f1_measure rdfs:comment \"F1-score computed per class and averaged weighted by class support; reflects overall performance when classes are imbalanced.\" .',\n",
    "    f':weighted_f1_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    f':confusion_matrix_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':confusion_matrix_measure rdfs:label \"Confusion Matrix\" .',\n",
    "    f':confusion_matrix_measure rdfs:comment \"Tabulates predicted vs. true class counts; supports interpretation of systematic misclassifications between popularity classes.\" .',\n",
    "    f':confusion_matrix_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "]\n",
    "\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def list_rf_hyperparameters():\n",
    "    rf = RandomForestClassifier()\n",
    "    params = rf.get_params()\n",
    "    num = 1\n",
    "\n",
    "    print(\"RandomForestClassifier list of hyperparameters:\\n\")\n",
    "    for key in sorted(params.keys()):\n",
    "        print(f\"{num}. {key}: {params[key]}\")\n",
    "        num += 1\n",
    "\n",
    "list_rf_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "model_data_code_writer = student_a\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "\n",
    "hp_comment = \"\"\"\n",
    "The RandomForestClassifier exposes several hyperparameters that influence model complexity,\n",
    "generalization behavior, robustness to noise, and computational cost. Prior to selecting a\n",
    "parameter for tuning, multiple hyperparameters were examined.\n",
    "\n",
    "The parameter n_estimators controls the number of trees in the ensemble. Increasing this value\n",
    "generally improves prediction stability but also increases training time, while performance\n",
    "improvements typically plateau beyond a certain number of trees. The max_features parameter\n",
    "determines how many features are considered at each split and influences the diversity of trees\n",
    "within the ensemble; although relevant in high-dimensional settings, it is often secondary once\n",
    "sufficient feature interactions are captured.\n",
    "\n",
    "The parameters min_samples_split and min_samples_leaf act as regularization mechanisms by limiting\n",
    "tree growth. Higher values enforce smoother decision boundaries and can reduce overfitting,\n",
    "particularly in the presence of noise or outliers. The class_weight parameter was also considered\n",
    "to address potential class imbalance by re-weighting misclassification penalties, which can improve\n",
    "macro-averaged evaluation metrics. The bootstrap parameter controls whether bootstrap sampling is\n",
    "applied and is typically left at its default value, as it rarely serves as the primary tuning\n",
    "dimension.\n",
    "\n",
    "Among these options, max_depth was selected as the most relevant hyperparameter for tuning.\n",
    "The max_depth parameter directly controls the complexity of individual decision trees: shallow\n",
    "trees may underfit complex, non-linear relationships between audio features, while very deep trees\n",
    "increase the risk of overfitting and computational cost. Tuning max_depth therefore provides a\n",
    "transparent and effective way to balance predictive performance, generalization, and training\n",
    "efficiency.\n",
    "\n",
    "The max_depth parameter is evaluated over a small set of discrete values representing increasing \n",
    "model complexity (e.g., 5, 10, 15, 20, and an unconstrained setting). This interval-based approach \n",
    "allows transparent analysis of the bias–variance tradeoff while keeping computational effort manageable.\n",
    "\"\"\"\n",
    "\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"Hyperparameter Identification and Selection for Tuning\" .',  # <- small addition\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Random Forest hyperparameters (identified)\n",
    "\n",
    "    f':hp_n_estimators rdf:type mls:HyperParameter .',\n",
    "    f':hp_n_estimators rdfs:label \"n_estimators\" .',\n",
    "    f':hp_n_estimators rdfs:comment \"Number of trees in the forest; improves stability but increases computational cost.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_n_estimators .',\n",
    "    f':hp_n_estimators prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    f':hp_max_depth rdf:type mls:HyperParameter .',\n",
    "    f':hp_max_depth rdfs:label \"max_depth\" .',\n",
    "    f':hp_max_depth rdfs:comment \"Maximum depth of individual trees; primary tuning parameter controlling model complexity and generalization.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_max_depth .',\n",
    "    f':hp_max_depth prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    f':hp_min_samples_leaf rdf:type mls:HyperParameter .',\n",
    "    f':hp_min_samples_leaf rdfs:label \"min_samples_leaf\" .',\n",
    "    f':hp_min_samples_leaf rdfs:comment \"Minimum samples required at a leaf node; acts as regularization to reduce overfitting.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_min_samples_leaf .',\n",
    "    f':hp_min_samples_leaf prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    f':hp_min_samples_split rdf:type mls:HyperParameter .',\n",
    "    f':hp_min_samples_split rdfs:label \"min_samples_split\" .',\n",
    "    f':hp_min_samples_split rdfs:comment \"Minimum samples required to split an internal node; limits tree growth and overfitting.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_min_samples_split .',\n",
    "    f':hp_min_samples_split prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    f':hp_max_features rdf:type mls:HyperParameter .',\n",
    "    f':hp_max_features rdfs:label \"max_features\" .',\n",
    "    f':hp_max_features rdfs:comment \"Number of features considered at each split; affects tree diversity and ensemble robustness.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_max_features .',\n",
    "    f':hp_max_features prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    f':hp_class_weight rdf:type mls:HyperParameter .',\n",
    "    f':hp_class_weight rdfs:label \"class_weight\" .',\n",
    "    f':hp_class_weight rdfs:comment \"Weights associated with classes; useful for handling class imbalance and improving macro-F1.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_class_weight .',\n",
    "    f':hp_class_weight prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    f':hp_bootstrap rdf:type mls:HyperParameter .',\n",
    "    f':hp_bootstrap rdfs:label \"bootstrap\" .',\n",
    "    f':hp_bootstrap rdfs:comment \"Controls whether bootstrap samples are used when building trees; typically left at default.\" .',\n",
    "    f':random_forest_classifier_implementation mls:hasHyperParameter :hp_bootstrap .',\n",
    "    f':hp_bootstrap prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df: pd.DataFrame, target_col: str, train_size: float, val_size: float, test_size: float, random_state: int):\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in dataframe.\")\n",
    "    if abs((train_size + val_size + test_size) - 1.0) > 1e-9:\n",
    "        raise ValueError(\"train_size + val_size + test_size must sum to 1.0\")\n",
    "\n",
    "    # 1) train vs temporary (val+test)\n",
    "    train_set, temp_set = train_test_split(\n",
    "        df,\n",
    "        test_size=(val_size + test_size),\n",
    "        random_state=random_state,\n",
    "        stratify=df[target_col]\n",
    "    )\n",
    "\n",
    "    # 2) val vs test from temp\n",
    "    test_frac_of_temp = test_size / (val_size + test_size)\n",
    "\n",
    "    validation_set, test_set = train_test_split(\n",
    "        temp_set,\n",
    "        test_size=test_frac_of_temp,\n",
    "        random_state=random_state,\n",
    "        stratify=temp_set[target_col]\n",
    "    )\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "spotify_data_path = os.path.join(\"data\")\n",
    "input_file = os.path.join(spotify_data_path, 'sample_100000_10_01_2026.csv')\n",
    "sample_data = pd.read_csv(input_file,  sep=',', header = 0)\n",
    "\n",
    "start_time_split = now()\n",
    "train_set, validation_set, test_set = split_data(sample_data, \"popularity_class\", 0.60, 0.20, 0.20, 42)\n",
    "end_time_split = now()\n",
    "\n",
    "n_train, n_validation, n_test = len(train_set), len(validation_set), len(test_set)\n",
    "print(\"Split sizes:\", n_train, n_validation, n_test)\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = f\"\"\"\n",
    "The prepared dataset was split into three disjoint subsets for supervised learning: a training set,\n",
    "a validation set, and a test set. Because the target variable consists of five popularity classes\n",
    "(Very bad, Bad, Average, Good, Very good), stratified sampling was applied to preserve the class\n",
    "distribution across all subsets.\n",
    "\n",
    "The split was defined as 60% training, 20% validation, and 20% test. A fixed random seed\n",
    "(random_state=42) was used to ensure reproducibility. No temporal or sequential dependencies\n",
    "between instances were assumed for this task, therefore a random stratified split was appropriate.\n",
    "\n",
    "Resulting subset sizes are: training={n_train}, validation={n_validation}, test={n_test}.\n",
    "\"\"\"\n",
    "\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":sample_100000_10_01_2026\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:label \"Train / Validation / Test Data Split\" .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:startedAtTime \"{start_time_split}\"^^xsd:dateTime .',\n",
    "    f':define_data_split prov:endedAtTime \"{end_time_split}\"^^xsd:dateTime .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "\n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains {n_train} samples\" .',\n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains {n_validation} samples\" .',\n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains {n_test} samples\" .',\n",
    "]\n",
    "\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def train_and_finetune_model(\n",
    "    training_set: pd.DataFrame,\n",
    "    validation_set: pd.DataFrame,\n",
    "    target_col: str = \"popularity_class\",\n",
    "    max_depth_grid=(5, 10, 15, 20, None),\n",
    "    base_params: dict | None = None,\n",
    "    output_fig_dir: str = \"figures\"\n",
    "):\n",
    "    if target_col not in training_set.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in training_set.\")\n",
    "    if target_col not in validation_set.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in validation_set.\")\n",
    "\n",
    "    os.makedirs(output_fig_dir, exist_ok=True)\n",
    "\n",
    "    # Split into X/y\n",
    "    X_train = training_set.drop(columns=[target_col], errors=\"ignore\").copy()\n",
    "    y_train = training_set[target_col].copy()\n",
    "\n",
    "    X_val = validation_set.drop(columns=[target_col], errors=\"ignore\").copy()\n",
    "    y_val = validation_set[target_col].copy()\n",
    "\n",
    "    # Drop high-cardinality / identifier columns (not used as features)\n",
    "    DROP_COLS = [\"id\", \"artist_name\", \"track_name\", \"track_id\"]\n",
    "    X_train = X_train.drop(columns=[c for c in DROP_COLS if c in X_train.columns], errors=\"ignore\")\n",
    "    X_val = X_val.drop(columns=[c for c in DROP_COLS if c in X_val.columns], errors=\"ignore\")\n",
    "\n",
    "    # Encoding (genre + any other categorical columns)\n",
    "    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    prov_triples = []\n",
    "\n",
    "    # Selection criterion: validation Macro-F1\n",
    "    best_model = None \n",
    "    best_depth = None\n",
    "    best_val_macro_f1 = -1.0\n",
    "\n",
    "    # Shared hyperparameters\n",
    "    base_params = base_params\n",
    "\n",
    "    fig_filename_1 = f\"rf_tuning_max_depth_{datetime.date.today().strftime('%Y_%m_%d')}.png\"\n",
    "    fig_path_1 = os.path.join(output_fig_dir, fig_filename_1)\n",
    "\n",
    "    fig_filename_2 = f\"rf_train_vs_validation_{datetime.date.today().strftime('%Y_%m_%d')}.png\"\n",
    "    fig_path_2 = os.path.join(output_fig_dir, fig_filename_2)\n",
    "\n",
    "    for i, depth in enumerate(max_depth_grid, start=1):\n",
    "        run_id = f\"run_{i}\"\n",
    "        model_id = f\"rf_model_{i}\"\n",
    "        hp_setting_id = f\"hp_setting_max_depth_{i}\"\n",
    "\n",
    "        # Train model (inside a pipeline with encoding)\n",
    "        clf = RandomForestClassifier(max_depth=depth, **base_params)\n",
    "        pipe = Pipeline(steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"model\", clf)\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        pred_train = pipe.predict(X_train)\n",
    "        pred_val = pipe.predict(X_val)\n",
    "\n",
    "        # Metrics\n",
    "        train_acc = accuracy_score(y_train, pred_train)\n",
    "        val_acc = accuracy_score(y_val, pred_val)\n",
    "\n",
    "        train_macro_f1 = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_macro_f1 = f1_score(y_val, pred_val, average=\"macro\")\n",
    "\n",
    "        train_weighted_f1 = f1_score(y_train, pred_train, average=\"weighted\")\n",
    "        val_weighted_f1 = f1_score(y_val, pred_val, average=\"weighted\")\n",
    "\n",
    "        rows.append({\n",
    "            \"run\": run_id,\n",
    "            \"max_depth\": depth if depth is not None else \"None\",\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"train_macro_f1\": train_macro_f1,\n",
    "            \"val_macro_f1\": val_macro_f1,\n",
    "            \"train_weighted_f1\": train_weighted_f1,\n",
    "            \"val_weighted_f1\": val_weighted_f1\n",
    "        })\n",
    "\n",
    "        # Select best by validation Macro-F1\n",
    "        if val_macro_f1 > best_val_macro_f1:\n",
    "            best_val_macro_f1 = val_macro_f1\n",
    "            best_model = pipe\n",
    "            best_depth = depth\n",
    "\n",
    "        depth_literal = '\"None\"' if depth is None else f'\"{float(depth)}\"^^xsd:double'\n",
    "        prov_triples.extend([\n",
    "            f':{hp_setting_id} rdf:type mls:HyperParameterSetting .',\n",
    "            f':{hp_setting_id} mls:specifiedBy :hp_max_depth .',\n",
    "            f':{hp_setting_id} mls:hasValue {depth_literal} .',\n",
    "            f':{hp_setting_id} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "\n",
    "            f':{run_id} rdf:type mls:Run .',\n",
    "            f':{run_id} sc:isPartOf :train_and_finetune_model .',\n",
    "            f':{run_id} mls:realizes :random_forest_algorithm .',\n",
    "            f':{run_id} rdfs:label \"Random Forest run {i} (max_depth={depth})\" .',\n",
    "            f':{run_id} mls:executes :random_forest_classifier_implementation .',\n",
    "            f':{run_id} mls:hasInput :training_set .',\n",
    "            f':{run_id} mls:hasInput :validation_set .',\n",
    "            f':{run_id} mls:hasInput :{hp_setting_id} .',\n",
    "            f':{run_id} mls:hasOutput :{model_id} .',\n",
    "\n",
    "            f':{model_id} rdf:type mls:Model .',\n",
    "            f':{model_id} rdfs:label \"RandomForestClassifier pipeline model from run {i}\" .',\n",
    "            f':{model_id} prov:wasGeneratedBy :{run_id} .',\n",
    "            f':{model_id} mlso:trainedOn :training_set .',\n",
    "            f':{model_id} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "        ])\n",
    "\n",
    "        eval_train_acc = f\"eval_train_acc_{i}\"\n",
    "        eval_val_acc = f\"eval_val_acc_{i}\"\n",
    "        eval_train_macro = f\"eval_train_macro_f1_{i}\"\n",
    "        eval_val_macro = f\"eval_val_macro_f1_{i}\"\n",
    "\n",
    "        prov_triples.extend([\n",
    "            f':{eval_train_acc} rdf:type mls:ModelEvaluation .',\n",
    "            f':{eval_train_acc} prov:wasGeneratedBy :{run_id} .',\n",
    "            f':{eval_train_acc} mls:hasValue \"{train_acc}\"^^xsd:double .',\n",
    "            f':{eval_train_acc} mls:specifiedBy :accuracy_measure .',\n",
    "            f':{eval_train_acc} prov:used :training_set .',\n",
    "\n",
    "            f':{eval_val_acc} rdf:type mls:ModelEvaluation .',\n",
    "            f':{eval_val_acc} prov:wasGeneratedBy :{run_id} .',\n",
    "            f':{eval_val_acc} mls:hasValue \"{val_acc}\"^^xsd:double .',\n",
    "            f':{eval_val_acc} mls:specifiedBy :accuracy_measure .',\n",
    "            f':{eval_val_acc} prov:used :validation_set .',\n",
    "\n",
    "            f':{eval_train_macro} rdf:type mls:ModelEvaluation .',\n",
    "            f':{eval_train_macro} prov:wasGeneratedBy :{run_id} .',\n",
    "            f':{eval_train_macro} mls:hasValue \"{train_macro_f1}\"^^xsd:double .',\n",
    "            f':{eval_train_macro} mls:specifiedBy :macro_f1_measure .',\n",
    "            f':{eval_train_macro} prov:used :training_set .',\n",
    "\n",
    "            f':{eval_val_macro} rdf:type mls:ModelEvaluation .',\n",
    "            f':{eval_val_macro} prov:wasGeneratedBy :{run_id} .',\n",
    "            f':{eval_val_macro} mls:hasValue \"{val_macro_f1}\"^^xsd:double .',\n",
    "            f':{eval_val_macro} mls:specifiedBy :macro_f1_measure .',\n",
    "            f':{eval_val_macro} prov:used :validation_set .',\n",
    "        ])\n",
    "\n",
    "    results_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Plot tuning curve\n",
    "    x_labels = results_df[\"max_depth\"].astype(str).tolist()\n",
    "    x = np.arange(len(x_labels))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, results_df[\"val_macro_f1\"].values, marker=\"o\")\n",
    "    plt.xticks(x, x_labels)\n",
    "    plt.xlabel(\"max_depth\")\n",
    "    plt.ylabel(\"Validation Macro-F1\")\n",
    "    plt.title(\"Random Forest hyperparameter tuning: max_depth vs Validation Macro-F1\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path_1, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    prov_triples.extend([\n",
    "        f':rf_tuning_plot rdf:type prov:Entity .',\n",
    "        f':rf_tuning_plot rdfs:label \"Tuning curve: max_depth vs validation Macro-F1\" .',\n",
    "        f':rf_tuning_plot prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "        f':rf_tuning_plot prov:atLocation \"{fig_path_1}\" .',\n",
    "    ])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, results_df[\"train_macro_f1\"].values, marker=\"o\", label=\"Train Macro-F1\")\n",
    "    plt.plot(x, results_df[\"val_macro_f1\"].values, marker=\"o\", label=\"Validation Macro-F1\")\n",
    "    plt.xticks(x, x_labels)\n",
    "    plt.xlabel(\"max_depth\")\n",
    "    plt.ylabel(\"Macro-F1\")\n",
    "    plt.title(\"Train vs Validation Macro-F1 across max_depth\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path_2, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    prov_triples.extend([\n",
    "    f':rf_tuning_train_vs_val_plot rdf:type prov:Entity .',\n",
    "    f':rf_tuning_train_vs_val_plot rdfs:label \"Train vs Validation Macro-F1 across max_depth\" .',\n",
    "    f':rf_tuning_train_vs_val_plot prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    f':rf_tuning_train_vs_val_plot prov:atLocation \"{fig_path_2}\" .',\n",
    "    ])\n",
    "\n",
    "    return best_model, best_depth, results_df, fig_path_1, prov_triples\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "base_params = {\n",
    "    \"n_estimators\": 200,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"bootstrap\": True,\n",
    "    \"criterion\": \"gini\"\n",
    "}\n",
    "\n",
    "train_25, _ = train_test_split(train_set, test_size=0.75, random_state=42, stratify=train_set[\"popularity_class\"])\n",
    "validation_25, _ = train_test_split(validation_set, test_size=0.75, random_state=42, stratify=validation_set[\"popularity_class\"])\n",
    "\n",
    "best_model, best_depth, tuning_results, fig_path_1, run_triples = train_and_finetune_model(\n",
    "    training_set=train_25,\n",
    "    validation_set=validation_25,\n",
    "    target_col=\"popularity_class\",\n",
    "    base_params=base_params\n",
    ")\n",
    "end_time_tafm = now()\n",
    "\n",
    "print(\"Best max_depth:\", best_depth)\n",
    "display(tuning_results)\n",
    "print(\"Best parameters:\")\n",
    "for p in base_params:\n",
    "    print(\"-\",p,\":\", base_params[p])\n",
    "print(\"- max_depth:\",best_depth)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = f\"\"\"\n",
    "Random Forest training and hyperparameter tuning were performed using a fixed train/validation split.\n",
    "Multiple model runs were executed by varying the hyperparameter max_depth over a predefined grid\n",
    "(max_depth ∈ 5, 10, 15, 20, None). All other model parameters were held constant across runs to ensure\n",
    "comparability and reproducibility.\n",
    "\n",
    "To enable experimentation with computationally more demanding configurations, the hyperparameter\n",
    "tuning experiments were conducted on stratified subsets containing 25% of the original training set\n",
    "and 25% of the original validation set. This subsampling strategy reduced computational overhead while\n",
    "preserving the class distribution and relative data characteristics. The reduced dataset size allowed\n",
    "the use of a larger ensemble size (n_estimators = {base_params[\"n_estimators\"]}) to improve model stability\n",
    "without prohibitive runtime costs.\n",
    "\n",
    "The following Random Forest parameters were fixed for all training runs: n_estimators = {base_params[\"n_estimators\"]},\n",
    "random_state = {base_params[\"random_state\"]}, n_jobs = {base_params[\"n_jobs\"]}, max_features = \"{base_params[\"max_features\"]}\",\n",
    "class_weight = \"{base_params[\"class_weight\"]}\", min_samples_leaf = {base_params[\"min_samples_leaf\"]},\n",
    "min_samples_split = {base_params[\"min_samples_split\"]}, bootstrap = {base_params[\"bootstrap\"]}, and criterion = \"{base_params[\"criterion\"]}\".\n",
    "Only the max_depth parameter was varied during tuning.\n",
    "\n",
    "For each run, a preprocessing-and-model pipeline was trained on the (subsampled) training set.\n",
    "Categorical attributes (e.g., genre) were one-hot encoded (handle_unknown=\"ignore\"), while numeric\n",
    "attributes were passed through unchanged. High-cardinality identifiers (artist_name, track_name,\n",
    "track_id) were excluded from the feature space. Although artist popularity is known to influence song\n",
    "success, artist identifiers were excluded to avoid identity-based memorization and to focus the model\n",
    "on generalizable musical characteristics. Track name and track identifiers were removed as they do not\n",
    "represent intrinsic musical properties relevant for popularity prediction.\n",
    "\n",
    "Model performance was evaluated on both the training and validation sets using Accuracy and\n",
    "Macro-averaged F1-score. Macro-F1 on the validation set served as the primary model selection criterion\n",
    "due to its robustness under potential class imbalance across the five popularity classes. Two figures\n",
    "were generated to support the analysis: (1) a tuning curve showing validation Macro-F1 across max_depth\n",
    "values, and (2) a plot comparing training vs. validation Macro-F1 to diagnose overfitting. The final\n",
    "hyperparameter setting was selected as the one achieving the highest validation Macro-F1 (selected\n",
    "max_depth = {best_depth}).\n",
    "\"\"\"\n",
    "\n",
    "train_model_activity = [\n",
    "\n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    f':training_set_25 rdf:type sc:Dataset .',\n",
    "    f':training_set_25 rdfs:label \"Training Subset (25%)\" .',\n",
    "    f':training_set_25 prov:wasDerivedFrom :training_set .',\n",
    "    f':training_set_25 rdfs:comment \"Stratified 25% subsample used for tuning experiments\" .',\n",
    "\n",
    "    f':validation_set_25 rdf:type sc:Dataset .',\n",
    "    f':validation_set_25 rdfs:label \"Validation Subset (25%)\" .',\n",
    "    f':validation_set_25 prov:wasDerivedFrom :validation_set .',\n",
    "    f':validation_set_25 rdfs:comment \"Stratified 25% subsample used for tuning experiments\" .',\n",
    "\n",
    "    f':train_and_finetune_model prov:used :training_set_25 .',\n",
    "    f':train_and_finetune_model prov:used :validation_set_25 .',\n",
    "\n",
    "    f':selected_model rdf:type prov:Entity .',\n",
    "    f':selected_model rdfs:label \"Selected model based on validation Macro-F1\" .',\n",
    "    f':selected_model prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    f':selected_model rdfs:comment \"Best max_depth chosen by highest validation Macro-F1 was: {best_depth}\" .',\n",
    "]\n",
    "\n",
    "train_model_activity.extend(run_triples)\n",
    "\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(\n",
    "    training_set: pd.DataFrame,\n",
    "    validation_set: pd.DataFrame,\n",
    "    target_col: str = \"popularity_class\",\n",
    "    best_depth=None,\n",
    "    base_params: dict | None = None,\n",
    "    drop_cols=None\n",
    "):\n",
    "    if drop_cols is None:\n",
    "        drop_cols = [\"id\", \"artist_name\", \"track_name\", \"track_id\"]\n",
    "\n",
    "    if base_params is None:\n",
    "        base_params = {\n",
    "            \"n_estimators\": 1000,\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"max_features\": \"sqrt\",\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"min_samples_leaf\": 2,\n",
    "            \"min_samples_split\": 4,\n",
    "            \"bootstrap\": True,\n",
    "            \"criterion\": \"gini\"\n",
    "        }\n",
    "\n",
    "    full_train = pd.concat([training_set, validation_set], axis=0).reset_index(drop=True)\n",
    "\n",
    "    X_full = full_train.drop(columns=[target_col], errors=\"ignore\").copy()\n",
    "    y_full = full_train[target_col].copy()\n",
    "\n",
    "    X_full = X_full.drop(columns=[c for c in drop_cols if c in X_full.columns], errors=\"ignore\")\n",
    "\n",
    "    cat_cols = X_full.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = [c for c in X_full.columns if c not in cat_cols]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=best_depth, **base_params)\n",
    "\n",
    "    final_model = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", clf)\n",
    "    ])\n",
    "\n",
    "    final_model.fit(X_full, y_full)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "start_time_retrain = now()\n",
    "final_model_pipeline = retrain_model_full_data(\n",
    "    training_set=train_set,\n",
    "    validation_set=validation_set,\n",
    "    target_col=\"popularity_class\",\n",
    "    best_depth=best_depth,\n",
    "    base_params=base_params\n",
    ")\n",
    "end_time_retrain = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "if best_depth is None:\n",
    "    best_depth_literal = '\"None\"'\n",
    "else:\n",
    "    best_depth_literal = f'\"{float(best_depth)}\"^^xsd:double'\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\"\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model_entity = \":final_model_entity\"\n",
    "\n",
    "retrain_comment = f\"\"\"\n",
    "The final model was retrained after hyperparameter selection using the full available training data,\n",
    "i.e., the union of the training and validation sets. Retraining was performed using the same\n",
    "preprocessing pipeline and identical hyperparameters as the best configuration identified during\n",
    "tuning (max_depth = {best_depth_literal}). Categorical features (e.g., genre) were one-hot encoded with\n",
    "handle_unknown=\"ignore\", numeric features were passed through unchanged, and identifier/high-cardinality\n",
    "attributes (artist_name, track_name, track_id) were excluded from the feature space.\n",
    "\n",
    "Fixed Random Forest parameters during retraining were: n_estimators = {base_params[\"n_estimators\"]},\n",
    "random_state = {base_params[\"random_state\"]}, n_jobs = {base_params[\"n_jobs\"]}, max_features = \"{base_params[\"max_features\"]}\",\n",
    "class_weight = \"{base_params[\"class_weight\"]}\", min_samples_leaf = {base_params[\"min_samples_leaf\"]},\n",
    "min_samples_split = {base_params[\"min_samples_split\"]}, bootstrap = {base_params[\"bootstrap\"]}, and criterion = \"{base_params[\"criterion\"]}\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "retrain_documentation = [\n",
    "    f'{final_training_activity} rdf:type prov:Activity .',\n",
    "    f'{final_training_activity} sc:isPartOf :modeling_phase .',\n",
    "    f'{final_training_activity} rdfs:comment \"Final model retraining on train+validation data\" .',\n",
    "    f'{final_training_activity} rdfs:comment \"\"\"{retrain_comment}\"\"\" .',\n",
    "    f'{final_training_activity} prov:startedAtTime \"{start_time_retrain}\"^^xsd:dateTime .',\n",
    "    f'{final_training_activity} prov:endedAtTime \"{end_time_retrain}\"^^xsd:dateTime .',\n",
    "    f'{final_training_activity} prov:qualifiedAssociation :{retrain_ass_uuid_writer} .',\n",
    "\n",
    "    f':{retrain_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{retrain_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{retrain_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    f'{final_training_activity} prov:used :training_set .',\n",
    "    f'{final_training_activity} prov:used :validation_set .',\n",
    "    f'{final_training_activity} prov:used :final_hp_max_depth .',\n",
    "\n",
    "    f':final_hp_max_depth rdf:type mls:HyperParameterSetting .',\n",
    "    f':final_hp_max_depth mls:specifiedBy :hp_max_depth .',\n",
    "    f':final_hp_max_depth mls:hasValue {best_depth_literal} .',\n",
    "    f':final_hp_max_depth prov:wasGeneratedBy {final_training_activity} .',\n",
    "\n",
    "    f'{final_model_entity} rdf:type mls:Model .',\n",
    "    f'{final_model_entity} rdfs:label \"Final RandomForestClassifier pipeline model (trained on train+validation)\" .',\n",
    "    f'{final_model_entity} prov:wasGeneratedBy {final_training_activity} .',\n",
    "    f'{final_model_entity} mlso:trainedOn :training_set .',\n",
    "    f'{final_model_entity} mlso:trainedOn :validation_set .',\n",
    "    f'{final_model_entity} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "]\n",
    "\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16e5b6-202c-4906-8af6-c9ecaea23053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "eval_code_writer = student_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7254-9e8a-416a-813e-d3d2fe524d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_data(final_model, test_df, target_col=\"popularity_class\"):\n",
    "    X_test = test_df.drop(columns=[target_col], errors=\"ignore\")\n",
    "    y_test = test_df[target_col]\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"macro_f1\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        \"weighted_f1\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ee5a5-1b3d-42a0-8cd9-15af6a4e8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class_baseline(test_df: pd.DataFrame, target_col: str = \"popularity_class\"):\n",
    "    if target_col not in test_df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in test set.\")\n",
    "\n",
    "    y_true = test_df[target_col]\n",
    "\n",
    "    majority_class = y_true.value_counts().idxmax()\n",
    "\n",
    "    y_pred = np.full(shape=len(y_true), fill_value=majority_class)\n",
    "\n",
    "    results = {\n",
    "        \"majority_class\": majority_class,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aac8de-585e-49a2-8320-de02d426508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_check_by_genre(final_model, test_df, target_col=\"popularity_class\"):\n",
    "    results = {}\n",
    "    for genre in test_df[\"genre\"].dropna().unique():\n",
    "        subset = test_df[test_df[\"genre\"] == genre]\n",
    "        if len(subset) < 50:\n",
    "            continue\n",
    "\n",
    "        X = subset.drop(columns=[target_col], errors=\"ignore\")\n",
    "        y = subset[target_col]\n",
    "        y_pred = final_model.predict(X)\n",
    "\n",
    "        results[genre] = accuracy_score(y, y_pred)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827661f-cd6a-4599-8453-56ff528aaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_eval = now()\n",
    "\n",
    "# Final model evaluation\n",
    "eval_results = evaluate_on_test_data(final_model_pipeline, test_set)\n",
    "\n",
    "# Majority-class baseline\n",
    "baseline_results = majority_class_baseline(test_set)\n",
    "\n",
    "end_time_eval = now()\n",
    "\n",
    "\n",
    "print(\"Final model results:\", eval_results)\n",
    "print(\"Majority class:\", baseline_results[\"majority_class\"])\n",
    "print(\"Baseline accuracy:\", baseline_results[\"accuracy\"])\n",
    "print(\"Baseline macro-F1:\", baseline_results[\"macro_f1\"])\n",
    "print(\"Baseline weighted-F1:\", baseline_results[\"weighted_f1\"])\n",
    "print(\"Baseline confusion matrix:\\n\", baseline_results[\"confusion_matrix\"])\n",
    "# print(\"Bias check (genre accuracy):\", bias_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e86e2-192d-4c2e-81ed-240031966e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 5 - Evaluation\n",
    "#############################################\n",
    "\n",
    "eval_comment = f\"\"\"\n",
    "The final Random Forest model was evaluated on a held-out test set to assess whether it meets the\n",
    "business and data mining objectives defined in the Business Understanding phase. Model performance\n",
    "was compared against a majority-class baseline, which always predicts the most frequent popularity\n",
    "class (“Very bad”). This baseline represents a simple heuristic that reflects the limitations of\n",
    "early-stage decision-making when no user engagement data is available.\n",
    "\n",
    "The majority-class baseline achieved an accuracy of 0.605 but a very low Macro-F1 score of 0.151,\n",
    "showing that it largely fails to identify tracks with above-average popularity and ignores minority\n",
    "classes. While such a heuristic may appear acceptable when considering accuracy alone, it does not\n",
    "support the business goal of reliably surfacing promising tracks early.\n",
    "\n",
    "In contrast, the final model achieved an accuracy of 0.996, a Macro-F1 score of 0.920, and a\n",
    "Weighted-F1 score of 0.996 on the test set. The strong improvement in Macro-F1 indicates that the\n",
    "model distinguishes well between all popularity classes rather than defaulting to the majority\n",
    "class. The confusion matrix shows that remaining errors mostly occur between neighboring popularity\n",
    "levels, which is acceptable for early-stage recommendation and curation support.\n",
    "\n",
    "These results satisfy the data mining goal of predicting popularity using only intrinsic audio\n",
    "features and metadata available at ingestion time. The model clearly outperforms a trivial baseline,\n",
    "demonstrating that meaningful early popularity estimates are possible and supporting the business\n",
    "objective of reducing manual curation and improving early promotional decisions.\n",
    "\n",
    "To assess potential bias, musical genre was treated as a protected attribute proxy, as it defines\n",
    "relevant subgroups of the catalogue that could be unfairly favored or disadvantaged. Genre-wise\n",
    "accuracy on the test set was consistently high, with no systematic performance degradation for any\n",
    "genre. Minor variations are explained by stylistic diversity rather than structural bias. Overall,\n",
    "no evidence of biased behavior toward specific genre groups was identified.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010defa-d86e-4549-acc2-f35c14630a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    ':evaluate_final_model rdf:type prov:Activity .',\n",
    "    ':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    ':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ':evaluate_final_model prov:used :final_model_entity .',\n",
    "    ':evaluate_final_model prov:used :test_set .',\n",
    "    ':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66c6c3-faf1-47f5-8c08-da3fe1e013f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_entities = [\n",
    "    ':test_accuracy rdf:type mls:ModelEvaluation .',\n",
    "    ':test_accuracy mls:specifiedBy :accuracy_measure .',\n",
    "    f':test_accuracy mls:hasValue \"{eval_results[\"accuracy\"]}\"^^xsd:double .',\n",
    "    ':test_accuracy prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    ':test_accuracy prov:used :test_set .',\n",
    "\n",
    "    ':test_macro_f1 rdf:type mls:ModelEvaluation .',\n",
    "    ':test_macro_f1 mls:specifiedBy :macro_f1_measure .',\n",
    "    f':test_macro_f1 mls:hasValue \"{eval_results[\"macro_f1\"]}\"^^xsd:double .',\n",
    "    ':test_macro_f1 prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    ':test_macro_f1 prov:used :test_set .',\n",
    "\n",
    "    ':test_weighted_f1 rdf:type mls:ModelEvaluation .',\n",
    "    ':test_weighted_f1 mls:specifiedBy :weighted_f1_measure .',\n",
    "    f':test_weighted_f1 mls:hasValue \"{eval_results[\"weighted_f1\"]}\"^^xsd:double .',\n",
    "    ':test_weighted_f1 prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    ':test_weighted_f1 prov:used :test_set .',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7350c19-79b9-4666-baef-663dba64a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_activity.extend(evaluation_entities)\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "The final model clearly outperforms a trivial majority-class baseline and meets the data mining\n",
    "objective of predicting popularity classes from audio features and metadata alone. This indicates\n",
    "that the model can support early-stage decisions before user engagement data is available.\n",
    "\n",
    "From a business perspective, the results are sufficient to assist playlist curation and promotional\n",
    "prioritization, but not to fully automate such decisions. A hybrid deployment is recommended, where\n",
    "the model is used to rank or filter tracks for further human review rather than to replace editorial\n",
    "judgment. Additional validation on newly released tracks would further strengthen deployment readiness.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "The model does not use explicit sensitive attributes, but genre was treated as a protected or proxy\n",
    "attribute because it reflects cultural groupings. Performance across genres was consistently high,\n",
    "without evidence of systematic bias.\n",
    "\n",
    "Nevertheless, there is a risk that popularity predictions could reinforce existing mainstream trends.\n",
    "To mitigate this, predictions should be used as guidance rather than as absolute decisions, and\n",
    "diversity considerations should remain part of the deployment process.\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "After deployment, model performance should be monitored using overall accuracy, macro-F1 score,\n",
    "and per-class performance on newly ingested tracks. Changes in prediction distributions or sustained\n",
    "drops in macro-F1 should trigger investigation or retraining.\n",
    "\n",
    "Comparing early predictions with later observed popularity outcomes is recommended to ensure the\n",
    "model remains aligned with real user engagement over time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "The workflow is largely reproducible, as preprocessing steps, hyperparameters, random seeds, and\n",
    "model choices are documented and linked through provenance information. This allows the experiment\n",
    "to be retraced with minimal ambiguity.\n",
    "\n",
    "Remaining risks include dataset sampling choices and dependency versions. These could be further\n",
    "reduced by fixing dataset snapshots and explicitly versioning all external libraries.\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "#engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "#engine.insert(deployment_data_executor, prefixes=prefixes)\n",
    "engine.insert(\n",
    "    deployment_executor + deployment_data_executor,\n",
    "    prefixes=prefixes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2642330b-b16c-4936-9718-a13d33f6d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe defaults for LaTeX placeholders\n",
    "eval_table_rows = \"\"\n",
    "evaluation_section = \"\"\n",
    "deployment_section = \"\"\n",
    "modeling_section = \"\"\n",
    "dp_section = \"\"\n",
    "du_analysis_sections = \"\"\n",
    "du_table_rows = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3492f667-1cf2-424f-aeda-c30b0589a9d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 110] Connection timed out>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1336\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1382\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1381\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1470\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1001\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1000\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:852\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[33m\"\u001b[39m\u001b[33mcreate_connection failed\u001b[39m\u001b[33m\"\u001b[39m, exceptions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:837\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    836\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Author Block (DEDUPLICATED & SAFE)\u001b[39;00m\n\u001b[32m      2\u001b[39m author_query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mprefix_header\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33mPREFIX iao: <http://purl.obolibrary.org/obo/>\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33mGROUP BY ?uri\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m res_authors = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m author_blocks = []  \u001b[38;5;66;03m# IMPORTANT: no +=\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res_authors.empty:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/starvers/starvers.py:413\u001b[39m, in \u001b[36mTripleStoreEngine.query\u001b[39m\u001b[34m(self, select_statement, timestamp, yn_timestamp_query, as_df)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m#self.sparql_get_with_post.queryType = 'SELECT'\u001b[39;00m\n\u001b[32m    412\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRetrieving results ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_get_with_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe result has the return type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult._get_responseFormat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m as_df:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/jmutv/Documents/GitHub/BI_Assignment_3/venv/lib/python3.12/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m     req = meth(req)\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    518\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1344\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1345\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m     r = h.getresponse()\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno 110] Connection timed out>"
     ]
    }
   ],
   "source": [
    "### Author Block (DEDUPLICATED & SAFE)\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT ?uri\n",
    "       (SAMPLE(?givenRaw) AS ?given)\n",
    "       (SAMPLE(?familyRaw) AS ?family)\n",
    "       (SAMPLE(?matrRaw) AS ?matr)\n",
    "WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  ?uri a foaf:Person ;\n",
    "       foaf:givenName ?givenRaw ;\n",
    "       foaf:familyName ?familyRaw ;\n",
    "       iao:IAO_0000219 ?matrRaw .\n",
    "}}\n",
    "GROUP BY ?uri\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "\n",
    "author_blocks = []  # IMPORTANT: no +=\n",
    "\n",
    "if not res_authors.empty:  # type: ignore\n",
    "    for _, row in res_authors.iterrows():  # type: ignore\n",
    "        uri_str = str(row[\"uri\"])\n",
    "        given = latex_escape(clean_rdf(row[\"given\"]))\n",
    "        family = latex_escape(clean_rdf(row[\"family\"]))\n",
    "        matr = latex_escape(clean_rdf(row[\"matr\"]))\n",
    "\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "\n",
    "        author_blocks.append(rf\"\"\"\n",
    "\\author{{{given} {family}}}\n",
    "\\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "\\affiliation{{\n",
    "  \\institution{{TU Wien}}\n",
    "  \\country{{Austria}}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "author_block_latex = \"\\n\".join(author_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ce45c-5998-45bf-be09-d2f929d717ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# BUSINESS UNDERSTANDING (OPTIMIZED, SINGLE-NODE SAFE)\n",
    "#######################################################################\n",
    "\n",
    "def get_single_comment(entity_iri: str) -> str:\n",
    "    q = f\"\"\"\n",
    "    {prefix_header}\n",
    "    SELECT ?c WHERE {{\n",
    "      {entity_iri} rdfs:comment ?c .\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    res = engine.query(q)\n",
    "    if res.empty:\n",
    "        return \"\"\n",
    "    return latex_escape(clean_rdf(res.iloc[0][\"c\"]))  # type: ignore\n",
    "\n",
    "\n",
    "bu_data_source = get_single_comment(\":bu_data_source_and_scenario\")\n",
    "bu_objectives = get_single_comment(\":bu_business_objectives\")\n",
    "bu_business_success = get_single_comment(\":bu_business_success_criteria\")\n",
    "bu_data_mining_goals = get_single_comment(\":bu_data_mining_goals\")\n",
    "bu_data_mining_success = get_single_comment(\":bu_data_mining_success_criteria\")\n",
    "bu_ai_risks = get_single_comment(\":bu_ai_risk_aspects\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e8561-48ce-4a47-9115-01bd8ddd8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# DATA UNDERSTANDING START\n",
    "#######################################################################\n",
    "\n",
    "du_analysis_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?entity ?label ?comment WHERE {{\n",
    "  VALUES ?entity {{\n",
    "    :data_volume_report\n",
    "    :du_skewness_analysis\n",
    "    :outlier_report\n",
    "  }}\n",
    "\n",
    "  ?entity rdfs:label ?label .\n",
    "  ?entity rdfs:comment ?comment .\n",
    "\n",
    "  # Keep ONLY the explanatory comment, drop short labels\n",
    "  FILTER(STRLEN(STR(?comment)) > 80)\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_du_analysis = engine.query(du_analysis_query)\n",
    "\n",
    "du_analysis_blocks = []\n",
    "\n",
    "if not res_du_analysis.empty:  # type: ignore\n",
    "    for _, row in res_du_analysis.iterrows():  # type: ignore\n",
    "        du_analysis_blocks.append(rf\"\"\"\n",
    "\\subsubsection{{{latex_escape(clean_rdf(row[\"label\"]))}}}\n",
    "{latex_escape(clean_rdf(row[\"comment\"]))}\n",
    "\"\"\")\n",
    "\n",
    "du_analysis_sections = \"\\n\".join(du_analysis_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb72a3-e27b-4797-b7f7-58e02decafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# DATA PREPARATION START\n",
    "#######################################################################\n",
    "\n",
    "### Data Preparation – Report (single authoritative comment)\n",
    "dp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT (SAMPLE(?commentRaw) AS ?comment) WHERE {{\n",
    "  :data_preparation rdfs:comment ?commentRaw .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_dp = engine.query(dp_query)\n",
    "\n",
    "dp_section = \"\"\n",
    "if not res_dp.empty:  # type: ignore\n",
    "    dp_section = latex_escape(\n",
    "        clean_rdf(res_dp.iloc[0][\"comment\"])\n",
    "    )  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f2aea-0b22-4da5-92e1-46e8719bfece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# MODELING START\n",
    "#######################################################################\n",
    "\n",
    "### Identified Hyperparameters (single authoritative list)\n",
    "hp_list_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?descRaw) AS ?desc) WHERE {{\n",
    "  :random_forest_classifier_implementation mls:hasHyperParameter ?hp .\n",
    "  ?hp rdfs:label ?name .\n",
    "  OPTIONAL {{ ?hp rdfs:comment ?descRaw . }}\n",
    "}}\n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "\n",
    "res_hp_list = engine.query(hp_list_query)\n",
    "\n",
    "hp_rows = []\n",
    "if not res_hp_list.empty:  # type: ignore\n",
    "    for _, row in res_hp_list.iterrows():  # type: ignore\n",
    "        hp_rows.append(\n",
    "            rf\"{latex_escape(clean_rdf(row['name']))} & \"\n",
    "            rf\"{latex_escape(clean_rdf(row.get('desc','')))} \\\\\"\n",
    "        )\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "\n",
    "### Modeling narrative (ONE comment per activity)\n",
    "mod_comment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?act (SAMPLE(?c) AS ?comment) WHERE {{\n",
    "  VALUES ?act {{\n",
    "    :define_algorithm\n",
    "    :identify_hyperparameters\n",
    "    :define_data_split\n",
    "    :train_and_finetune_model\n",
    "    :retrain_final_model\n",
    "  }}\n",
    "  ?act rdfs:comment ?c .\n",
    "}}\n",
    "GROUP BY ?act\n",
    "ORDER BY ?act\n",
    "\"\"\"\n",
    "\n",
    "res_mod_comments = engine.query(mod_comment_query)\n",
    "\n",
    "modeling_section = \"\"\n",
    "if not res_mod_comments.empty:  # type: ignore\n",
    "    for _, row in res_mod_comments.iterrows():  # type: ignore\n",
    "        act_name = str(row[\"act\"]).split(\"#\")[-1]\n",
    "        modeling_section += rf\"\"\"\n",
    "\\subsection{{{latex_escape(act_name.replace(\"_\",\" \").title())}}}\n",
    "{latex_escape(clean_rdf(row[\"comment\"]))}\n",
    "\"\"\"\n",
    "\n",
    "#######################################################################\n",
    "# MODELING END\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# EVALUATION START\n",
    "#######################################################################\n",
    "\n",
    "### Evaluation report (single authoritative comment)\n",
    "eval_comment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT (SAMPLE(?c) AS ?comment) WHERE {{\n",
    "  :evaluate_final_model rdfs:comment ?c .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "res_eval_comment = engine.query(eval_comment_query)\n",
    "\n",
    "evaluation_section = \"\"\n",
    "if not res_eval_comment.empty:  # type: ignore\n",
    "    evaluation_section = latex_escape(\n",
    "        clean_rdf(res_eval_comment.iloc[0][\"comment\"])\n",
    "    )  # type: ignore\n",
    "\n",
    "#######################################################################\n",
    "# EVALUATION END\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# DEPLOYMENT START\n",
    "#######################################################################\n",
    "\n",
    "### Deployment sections (one node = one subsection)\n",
    "deployment_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?entity ?label (SAMPLE(?commentRaw) AS ?comment) WHERE {{\n",
    "  VALUES ?entity {{\n",
    "    :dep_recommendations\n",
    "    :dep_ethical_risks\n",
    "    :dep_monitoring_plan\n",
    "    :dep_reproducibility_reflection\n",
    "  }}\n",
    "  ?entity rdfs:label ?label ;\n",
    "          rdfs:comment ?commentRaw .\n",
    "}}\n",
    "GROUP BY ?entity ?label\n",
    "ORDER BY ?entity\n",
    "\"\"\"\n",
    "\n",
    "deployment_section = \"\"\n",
    "\n",
    "res_dep = engine.query(deployment_query)\n",
    "\n",
    "if not res_dep.empty:  # type: ignore\n",
    "    for _, row in res_dep.iterrows():  # type: ignore\n",
    "        deployment_section += rf\"\"\"\n",
    "\\subsection{{{latex_escape(clean_rdf(row['label']))}}}\n",
    "{latex_escape(clean_rdf(row['comment']))}\n",
    "\"\"\"\n",
    "else:\n",
    "    deployment_section = r\"\\textit{No deployment information was found in the knowledge graph.}\"\n",
    "\n",
    "#######################################################################\n",
    "# DEPLOYMENT END\n",
    "#######################################################################\n",
    "\n",
    "print(\"Data extraction done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "\\subsection{{Business Success Criteria}}\n",
    "{bu_business_success}\n",
    "\n",
    "\\subsection{{Data Mining Goals}}\n",
    "{bu_data_mining_goals}\n",
    "\n",
    "\\subsection{{Data Mining Success Criteria}}\n",
    "{bu_data_mining_success}\n",
    "\n",
    "\\subsection{{AI Risk Aspects}}\n",
    "{bu_ai_risks}\n",
    "\n",
    "\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "{du_analysis_sections}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "{dp_section}\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "{modeling_section}\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Identified Random Forest Hyperparameters}}\n",
    "  \\label{{tab:rf_hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.75\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Hyperparameter}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "{evaluation_section}\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Final Model Performance on Test Set}}\n",
    "  \\label{{tab:testmetrics}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Metric}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {eval_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "{deployment_section}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latex_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m out_path = os.path.join(out_dir, \u001b[33m\"\u001b[39m\u001b[33mexperiment_report.tex\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     f.write(\u001b[43mlatex_content\u001b[49m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReport written to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'latex_content' is not defined"
     ]
    }
   ],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5347bb4-87fa-4761-b81d-a20b37d35db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d513f54-ee19-491f-9aa2-e824040b10cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
