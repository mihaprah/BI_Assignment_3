{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12434660'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '02'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12434660'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12440619'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4080a558",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     13\u001b[39m reigstration_triples_b = [\n\u001b[32m     14\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type foaf:Person .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Agent .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <http://purl.obolibrary.org/obo/IAO_0000219> \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m12440619\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^^xsd:string .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n\u001b[32m     24\u001b[39m role_triples = [\n\u001b[32m     25\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_writer_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreigstration_triples_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m engine.insert(reigstration_triples_b, prefixes=prefixes)\n\u001b[32m     32\u001b[39m engine.insert(role_triples, prefixes=prefixes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/starvers/starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Miha\" .',\n",
    "f':{student_a} foaf:familyName \"Prah\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12434660\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Jakov\" .',\n",
    "f':{student_b} foaf:familyName \"Mutvar\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"12440619\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "890a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data_path = os.path.join(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Each Activity that follows is part of the Business Understanding Phase\u001b[39;00m\n\u001b[32m      3\u001b[39m business_understanding_phase_executor = [\n\u001b[32m      4\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:business_understanding_phase rdf:type prov:Activity .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:business_understanding_phase rdfs:label \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBusiness Understanding Phase\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m## Phase 1: Business Understanding\u001b[39;00m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbusiness_understanding_phase_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/starvers/starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     66\u001b[39m bu_ass_uuid_executor = \u001b[33m\"\u001b[39m\u001b[33mbb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Generate once\u001b[39;00m\n\u001b[32m     67\u001b[39m business_understanding_executor = [\n\u001b[32m     68\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:business_understanding rdf:type prov:Activity .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     69\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:business_understanding sc:isPartOf :business_understanding_phase .\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;66;03m# Connect Activity to Parent Business Understanding Phase Activity\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbu_ass_uuid_executor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prov:hadRole :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     74\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbusiness_understanding_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m business_understanding_data_executor = [\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# 1a\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:bu_data_source_and_scenario rdf:type prov:Entity .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m ]\n\u001b[32m    111\u001b[39m engine.insert(business_understanding_data_executor, prefixes=prefixes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/starvers/starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The dataset used in this project is the Spotify 1 Million Tracks collection obtained from Kaggle, containing roughly\n",
    "one million songs with detailed metadata—such as artist name, track title, release year, genre, and an engagement-based\n",
    "popularity score—alongside Spotify’s engineered audio features, including danceability, energy, loudness, acousticness, instrumentalness,\n",
    "valence, tempo, and duration. These attributes describe the intrinsic characteristics of each track independently of user behaviour and\n",
    "therefore allow the construction of models that attempt to estimate popularity solely from acoustic and contextual properties.\n",
    "The business scenario motivating this analysis is that of a music-streaming platform seeking to evaluate newly ingested tracks before\n",
    "any substantial listening history exists: because early playlist placement and promotion strongly influence long-term performance, the\n",
    "platform requires a data-driven mechanism to identify promising tracks based only on their audio profile and metadata. The dataset aligns\n",
    "directly with this need, providing a large, diverse basis for analysing whether a track’s inherent musical properties can serve as reliable\n",
    "predictors of its eventual popularity.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The primary objective of the streaming platform in this scenario is to strengthen its early decision-making for newly released\n",
    "or newly ingested tracks, for which no meaningful user engagement data is yet available. By predicting the likelihood that a track\n",
    "will achieve above-average popularity using only its intrinsic acoustic and metadata attributes, the platform aims to improve the\n",
    "efficiency of its playlist-curation and recommendation processes. More accurate early assessments enable the platform to allocate promotional\n",
    "exposure more selectively, reduce dependence on manual curation, and increase listener engagement by prioritizing content with high\n",
    "potential impact. Ultimately, the objective is to support more effective catalogue management in a context where the volume of incoming\n",
    "tracks exceeds the platform’s capacity for human evaluation.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "Business success in this context is defined by measurable improvements in how the platform identifies and promotes promising tracks\n",
    "before user engagement signals accumulate. Success would be reflected in higher downstream listener engagement for tracks selected through\n",
    "the predictive system compared with those promoted under existing heuristics, as well as reductions in manual curation effort due to increased\n",
    "automation of early-stage selection. Additionally, successful deployment would lead to more efficient allocation of promotional resources,\n",
    "observable through improved performance of curated playlists or early-exposure campaigns. These outcomes must be attributable to the predictive\n",
    "system’s ability to surface high-potential tracks earlier and more consistently than current operational processes.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "The central data mining goal is to construct and evaluate a predictive model that estimates a track’s future popularity class\n",
    "based solely on the attributes available at the time of ingestion, namely its audio features and metadata. This involves identifying\n",
    "which features contribute most strongly to popularity outcomes, determining whether popularity can be reliably inferred from a track’s\n",
    "intrinsic characteristics, and quantifying the model’s ability to generalize across diverse genres and time periods. Beyond predictive\n",
    "accuracy, the analysis also seeks to generate interpretable insights into the relationship between musical properties and commercial performance.\n",
    "The overarching goal is to determine whether such a model can meaningfully support the platform’s early-stage decision-making process.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "The success of the data mining effort is assessed through model-based performance metrics that quantify how reliably popularity can\n",
    "be predicted from the available features. Suitable criteria include achieving a classification performance that clearly exceeds a trivial\n",
    "or random baseline, maintaining stable results across validation folds, and demonstrating adequate sensitivity to tracks in the higher-popularity\n",
    "classes, as these are the cases of greatest business interest. The model should show consistent behaviour across genres and release years, indicating\n",
    "that predictive patterns are not confined to narrow subsets of the data. In addition, the resulting feature-importance patterns or model explanations\n",
    "should be coherent with domain understanding and provide actionable insights into the drivers of popularity.\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "Several AI-related risks must be considered in this scenario. Because popularity is strongly influenced by prior exposure and historical\n",
    "preference patterns, a model trained on such data may inadvertently reinforce existing biases—for example, favouring established artists\n",
    "or mainstream genres while disadvantaging niche or underrepresented categories. The dataset lacks demographic or contextual interaction\n",
    "data, making it difficult to detect or mitigate such systemic effects. There is also a risk of temporal drift, as musical tastes and platform\n",
    "dynamics evolve, potentially degrading model performance over time if not monitored. Finally, deploying a popularity-prediction model introduces\n",
    "the possibility of creating self-fulfilling feedback loops, where the system boosts tracks it predicts to be successful, thereby influencing\n",
    "the very outcome it attempts to measure. These risks necessitate careful evaluation and ongoing monitoring before operational use.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\" # Generate once\n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>449566</th>\n",
       "      <td>449566</td>\n",
       "      <td>Steve Petrunak</td>\n",
       "      <td>Streams (For All the Saints) [Instrumental Ver...</td>\n",
       "      <td>1arz2xoQ9qCRpf4lTS89Py</td>\n",
       "      <td>14</td>\n",
       "      <td>2020</td>\n",
       "      <td>guitar</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.198</td>\n",
       "      <td>6</td>\n",
       "      <td>-13.053</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.882000</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.137</td>\n",
       "      <td>84.698</td>\n",
       "      <td>268751</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531354</th>\n",
       "      <td>531354</td>\n",
       "      <td>Soltune</td>\n",
       "      <td>Favor</td>\n",
       "      <td>62SfxrGZChnlj7JMgUj7X8</td>\n",
       "      <td>15</td>\n",
       "      <td>2022</td>\n",
       "      <td>afrobeat</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.827</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.059</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.914</td>\n",
       "      <td>179.898</td>\n",
       "      <td>194220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803660</th>\n",
       "      <td>933773</td>\n",
       "      <td>Mortician</td>\n",
       "      <td>Chainsaw Dismemberment - Live in Houston, TX O...</td>\n",
       "      <td>51RtroXai7GudO6oLrXAIb</td>\n",
       "      <td>3</td>\n",
       "      <td>2004</td>\n",
       "      <td>death-metal</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.913</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.388</td>\n",
       "      <td>75.977</td>\n",
       "      <td>107040</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844136</th>\n",
       "      <td>994119</td>\n",
       "      <td>Todd Barry</td>\n",
       "      <td>Buy Parents House</td>\n",
       "      <td>2jtzebhhZYBseK3eFeZ16k</td>\n",
       "      <td>14</td>\n",
       "      <td>2005</td>\n",
       "      <td>comedy</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.773</td>\n",
       "      <td>7</td>\n",
       "      <td>-13.857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.826</td>\n",
       "      <td>144.377</td>\n",
       "      <td>61093</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834975</th>\n",
       "      <td>982199</td>\n",
       "      <td>Lars Winnerbäck</td>\n",
       "      <td>Se dig om</td>\n",
       "      <td>5M52CJgSohdGyYy6Tgnsxf</td>\n",
       "      <td>26</td>\n",
       "      <td>2004</td>\n",
       "      <td>swedish</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.193</td>\n",
       "      <td>4</td>\n",
       "      <td>-12.007</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.452</td>\n",
       "      <td>99.662</td>\n",
       "      <td>194560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id      artist_name  \\\n",
       "449566  449566   Steve Petrunak   \n",
       "531354  531354          Soltune   \n",
       "803660  933773        Mortician   \n",
       "844136  994119       Todd Barry   \n",
       "834975  982199  Lars Winnerbäck   \n",
       "\n",
       "                                               track_name  \\\n",
       "449566  Streams (For All the Saints) [Instrumental Ver...   \n",
       "531354                                              Favor   \n",
       "803660  Chainsaw Dismemberment - Live in Houston, TX O...   \n",
       "844136                                  Buy Parents House   \n",
       "834975                                          Se dig om   \n",
       "\n",
       "                      track_id  popularity  year        genre  danceability  \\\n",
       "449566  1arz2xoQ9qCRpf4lTS89Py          14  2020       guitar         0.328   \n",
       "531354  62SfxrGZChnlj7JMgUj7X8          15  2022     afrobeat         0.719   \n",
       "803660  51RtroXai7GudO6oLrXAIb           3  2004  death-metal         0.329   \n",
       "844136  2jtzebhhZYBseK3eFeZ16k          14  2005       comedy         0.516   \n",
       "834975  5M52CJgSohdGyYy6Tgnsxf          26  2004      swedish         0.463   \n",
       "\n",
       "        energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "449566   0.198    6   -13.053     1       0.0466         0.961   \n",
       "531354   0.827    6    -2.059     1       0.3590         0.263   \n",
       "803660   0.852    0    -9.913     1       0.1500         0.136   \n",
       "844136   0.773    7   -13.857     0       0.9550         0.790   \n",
       "834975   0.193    4   -12.007     0       0.0302         0.555   \n",
       "\n",
       "        instrumentalness  liveness  valence    tempo  duration_ms  \\\n",
       "449566          0.882000     0.101    0.137   84.698       268751   \n",
       "531354          0.000375     0.560    0.914  179.898       194220   \n",
       "803660          0.815000     0.911    0.388   75.977       107040   \n",
       "844136          0.000000     0.873    0.826  144.377        61093   \n",
       "834975          0.003920     0.372    0.452   99.662       194560   \n",
       "\n",
       "        time_signature  \n",
       "449566               4  \n",
       "531354               3  \n",
       "803660               4  \n",
       "844136               4  \n",
       "834975               4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_spotify_data_code_writer = student_a\n",
    "def load_spotify_data()-> pd.DataFrame:\n",
    "\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(spotify_data_path, 'spotify.csv')\n",
    "    raw_data = pd.read_csv(input_file,  sep=',', header = 0)\n",
    "    sampled = raw_data.sample(n=2000, random_state=None)\n",
    "\n",
    "    save_new_sample = False\n",
    "\n",
    "    if(save_new_sample): \n",
    "        sampled.to_csv(\"data/sampled.csv\", index=False)\n",
    "    \n",
    "    return sampled\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_spotify_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"b8bac193-c4e6-4e31-9134-b23e001e279c\" # Generate once\n",
    "load_spotify_data_executor = [\n",
    "    f':load_spotify_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "# engine.insert(load_spotify_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"c600e15c-87a9-4e2a-be85-b6c2a3014210\" # Generate once\n",
    "ld_report = \"\"\"\n",
    "Load all Spotify data and create a sampled version of only 2000 instances. Sampled version can be optionaly saved in the data folder. \n",
    "\"\"\"\n",
    "load_spotify_data_activity = [\n",
    "    ':load_spotify_data rdf:type prov:Activity .',\n",
    "    ':load_spotify_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_spotify_data rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':load_spotify_data rdfs:comment \"\"\"{ld_report}\"\"\" .', \n",
    "    f':load_spotify_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_spotify_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_spotify_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_spotify_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT of activity\n",
    "    ':load_spotify_data prov:used :raw_data .',\n",
    "    ':load_spotify_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_spotify_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "# engine.insert(load_spotify_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further descibe the raw data using Croissant\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \\'Spotify 1 Million Tracks Dataset\\' .',\n",
    "    ':raw_data sc:description \\'Dataset containing metadata and audio features for approximately one million Spotify tracks.\\' .',\n",
    "\n",
    "    # File object describing the CSV source\n",
    "    ':spotify_csv rdf:type cr:FileObject .',\n",
    "    ':spotify_csv sc:name \\'spotify_tracks.csv\\' .',\n",
    "    ':spotify_csv sc:encodingFormat \\'text/csv\\' .',\n",
    "    ':raw_data sc:distribution :spotify_csv .',\n",
    "\n",
    "    # RecordSet containing the table\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \\'Spotify tracks recordset\\' .',\n",
    "    ':raw_recordset cr:source :spotify_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "\n",
    "    # ---------------------- FIELD DEFINITIONS ----------------------\n",
    "\n",
    "    # ID field\n",
    "    ':raw_recordset cr:field :field_id .',\n",
    "    ':field_id rdf:type cr:Field .',\n",
    "    ':field_id sc:name \\'ID\\' .',\n",
    "    ':field_id sc:description \\'Unique index of the track in the dataset.\\' .',\n",
    "    ':field_id cr:dataType xsd:integer .',\n",
    "\n",
    "    # Artist name\n",
    "    ':raw_recordset cr:field :field_artist .',\n",
    "    ':field_artist rdf:type cr:Field .',\n",
    "    ':field_artist sc:name \\'artist_name\\' .',\n",
    "    ':field_artist sc:description \\'Name of the artist associated with the track.\\' .',\n",
    "    ':field_artist cr:dataType xsd:string .',\n",
    "\n",
    "    # Track name\n",
    "    ':raw_recordset cr:field :field_track_name .',\n",
    "    ':field_track_name rdf:type cr:Field .',\n",
    "    ':field_track_name sc:name \\'track_name\\' .',\n",
    "    ':field_track_name sc:description \\'Title of the track.\\' .',\n",
    "    ':field_track_name cr:dataType xsd:string .',\n",
    "\n",
    "    # Track ID\n",
    "    ':raw_recordset cr:field :field_track_id .',\n",
    "    ':field_track_id rdf:type cr:Field .',\n",
    "    ':field_track_id sc:name \\'track_id\\' .',\n",
    "    ':field_track_id sc:description \\'Spotify identifier of the track.\\' .',\n",
    "    ':field_track_id cr:dataType xsd:string .',\n",
    "\n",
    "    # Popularity\n",
    "    ':raw_recordset cr:field :field_popularity .',\n",
    "    ':field_popularity rdf:type cr:Field .',\n",
    "    ':field_popularity sc:name \\'popularity\\' .',\n",
    "    ':field_popularity sc:description \\'Spotify popularity score from 0 to 100.\\' .',\n",
    "    ':field_popularity cr:dataType xsd:integer .',\n",
    "\n",
    "    # Year\n",
    "    ':raw_recordset cr:field :field_year .',\n",
    "    ':field_year rdf:type cr:Field .',\n",
    "    ':field_year sc:name \\'year\\' .',\n",
    "    ':field_year sc:description \\'Release year of the track.\\' .',\n",
    "    ':field_year cr:dataType xsd:gYear .',\n",
    "\n",
    "    # Genre\n",
    "    ':raw_recordset cr:field :field_genre .',\n",
    "    ':field_genre rdf:type cr:Field .',\n",
    "    ':field_genre sc:name \\'genre\\' .',\n",
    "    ':field_genre sc:description \\'Genre label assigned to the track.\\' .',\n",
    "    ':field_genre cr:dataType xsd:string .',\n",
    "\n",
    "    # Danceability\n",
    "    ':raw_recordset cr:field :field_danceability .',\n",
    "    ':field_danceability rdf:type cr:Field .',\n",
    "    ':field_danceability sc:name \\'danceability\\' .',\n",
    "    ':field_danceability sc:description \\'Suitability of a track for dancing (0.0 to 1.0).\\'.',\n",
    "    ':field_danceability cr:dataType xsd:double .',\n",
    "\n",
    "    # Energy\n",
    "    ':raw_recordset cr:field :field_energy .',\n",
    "    ':field_energy rdf:type cr:Field .',\n",
    "    ':field_energy sc:name \\'energy\\' .',\n",
    "    ':field_energy sc:description \\'Perceptual measure of intensity and activity (0.0 to 1.0).\\'.',\n",
    "    ':field_energy cr:dataType xsd:double .',\n",
    "\n",
    "    # Key\n",
    "    ':raw_recordset cr:field :field_key .',\n",
    "    ':field_key rdf:type cr:Field .',\n",
    "    ':field_key sc:name \\'key\\' .',\n",
    "    ':field_key sc:description \\'Estimated musical key of the track, encoded as integers 0–11.\\'.',\n",
    "    ':field_key cr:dataType xsd:integer .',\n",
    "\n",
    "    # Loudness\n",
    "    ':raw_recordset cr:field :field_loudness .',\n",
    "    ':field_loudness rdf:type cr:Field .',\n",
    "    ':field_loudness sc:name \\'loudness\\' .',\n",
    "    ':field_loudness sc:description \\'Overall loudness of the track in decibels (approx. -60 to 0).\\'.',\n",
    "    ':field_loudness cr:dataType xsd:double .',\n",
    "\n",
    "    # Mode\n",
    "    ':raw_recordset cr:field :field_mode .',\n",
    "    ':field_mode rdf:type cr:Field .',\n",
    "    ':field_mode sc:name \\'mode\\' .',\n",
    "    ':field_mode sc:description \\'Modality of the track: Major (1) or Minor (0).\\'.',\n",
    "    ':field_mode cr:dataType xsd:integer .',\n",
    "\n",
    "    # Speechiness\n",
    "    ':raw_recordset cr:field :field_speechiness .',\n",
    "    ':field_speechiness rdf:type cr:Field .',\n",
    "    ':field_speechiness sc:name \\'speechiness\\' .',\n",
    "    ':field_speechiness sc:description \\'Presence of spoken words in the track.\\'.',\n",
    "    ':field_speechiness cr:dataType xsd:double .',\n",
    "\n",
    "    # Acousticness\n",
    "    ':raw_recordset cr:field :field_acousticness .',\n",
    "    ':field_acousticness rdf:type cr:Field .',\n",
    "    ':field_acousticness sc:name \\'acousticness\\' .',\n",
    "    ':field_acousticness sc:description \\'Confidence measure of whether the track is acoustic (0.0 to 1.0).\\'.',\n",
    "    ':field_acousticness cr:dataType xsd:double .',\n",
    "\n",
    "    # Instrumentalness\n",
    "    ':raw_recordset cr:field :field_instrumentalness .',\n",
    "    ':field_instrumentalness rdf:type cr:Field .',\n",
    "    ':field_instrumentalness sc:name \\'instrumentalness\\' .',\n",
    "    ':field_instrumentalness sc:description \\'Likelihood the track contains no vocals (0.0 to 1.0).\\'.',\n",
    "    ':field_instrumentalness cr:dataType xsd:double .',\n",
    "\n",
    "    # Liveness\n",
    "    ':raw_recordset cr:field :field_liveness .',\n",
    "    ':field_liveness rdf:type cr:Field .',\n",
    "    ':field_liveness sc:name \\'liveness\\' .',\n",
    "    ':field_liveness sc:description \\'Probability the track was recorded live.\\'.',\n",
    "    ':field_liveness cr:dataType xsd:double .',\n",
    "\n",
    "    # Valence\n",
    "    ':raw_recordset cr:field :field_valence .',\n",
    "    ':field_valence rdf:type cr:Field .',\n",
    "    ':field_valence sc:name \\'valence\\' .',\n",
    "    ':field_valence sc:description \\'Musical positiveness conveyed by the track.\\'.',\n",
    "    ':field_valence cr:dataType xsd:double .',\n",
    "\n",
    "    # Tempo\n",
    "    ':raw_recordset cr:field :field_tempo .',\n",
    "    ':field_tempo rdf:type cr:Field .',\n",
    "    ':field_tempo sc:name \\'tempo\\' .',\n",
    "    ':field_tempo sc:description \\'Tempo of the track in beats per minute (BPM).\\'.',\n",
    "    ':field_tempo cr:dataType xsd:double .',\n",
    "\n",
    "    # Duration\n",
    "    ':raw_recordset cr:field :field_duration .',\n",
    "    ':field_duration rdf:type cr:Field .',\n",
    "    ':field_duration sc:name \\'duration_ms\\' .',\n",
    "    ':field_duration sc:description \\'Track duration in milliseconds.\\'.',\n",
    "    ':field_duration cr:dataType xsd:integer .',\n",
    "\n",
    "    # Time signature\n",
    "    ':raw_recordset cr:field :field_time_signature .',\n",
    "    ':field_time_signature rdf:type cr:Field .',\n",
    "    ':field_time_signature sc:name \\'time_signature\\' .',\n",
    "    ':field_time_signature sc:description \\'Estimated time signature which indicates the number of beats in each bar (e.g., 3 to 7).\\'.',\n",
    "    ':field_time_signature cr:dataType xsd:integer .',\n",
    "]\n",
    "# engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also the output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "\n",
    "    # Reuse all fields\n",
    "    ':recordset cr:field :field_id .',\n",
    "    ':recordset cr:field :field_artist .',\n",
    "    ':recordset cr:field :field_track_name .',\n",
    "    ':recordset cr:field :field_track_id .',\n",
    "    ':recordset cr:field :field_popularity .',\n",
    "    ':recordset cr:field :field_year .',\n",
    "    ':recordset cr:field :field_genre .',\n",
    "    ':recordset cr:field :field_danceability .',\n",
    "    ':recordset cr:field :field_energy .',\n",
    "    ':recordset cr:field :field_key .',\n",
    "    ':recordset cr:field :field_loudness .',\n",
    "    ':recordset cr:field :field_mode .',\n",
    "    ':recordset cr:field :field_speechiness .',\n",
    "    ':recordset cr:field :field_acousticness .',\n",
    "    ':recordset cr:field :field_instrumentalness .',\n",
    "    ':recordset cr:field :field_liveness .',\n",
    "    ':recordset cr:field :field_valence .',\n",
    "    ':recordset cr:field :field_tempo .',\n",
    "    ':recordset cr:field :field_duration .',\n",
    "    ':recordset cr:field :field_time_signature .',\n",
    "]\n",
    "# engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to the fields\n",
    "units_triples = [\n",
    "    ':field_loudness qudt:unit siu:decibel .',\n",
    "    ':field_tempo qudt:unit qudt:BeatsPerMinute .',\n",
    "    ':field_duration qudt:unit siu:millisecond .',\n",
    "    ':field_danceability qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_energy qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_acousticness qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_instrumentalness qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_valence qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_liveness qudt:unit qudt:DimensionlessUnit .',\n",
    "    ':field_speechiness qudt:unit qudt:DimensionlessUnit .',\n",
    "]\n",
    "# engine.insert(units_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0580e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danceability: 675 unique values (column had outliers)\n",
      "speechiness: 795 unique values (column had outliers)\n",
      "liveness: 826 unique values (column had outliers)\n",
      "Total unique outlier IDs across ALL columns: 129\n",
      "{'year': [], 'danceability': [{'index': 85, 'z_score': -2.9827874051127283}, {'index': 488, 'z_score': -2.5397318450972803}, {'index': 503, 'z_score': -2.578185723891074}, {'index': 1257, 'z_score': -2.9827874051127283}, {'index': 1422, 'z_score': -2.9827874051127283}, {'index': 1439, 'z_score': -2.9827874051127283}, {'index': 1713, 'z_score': -2.638931706333444}, {'index': 1881, 'z_score': -2.640603614107087}, {'index': 1972, 'z_score': -2.9827874051127283}], 'energy': [], 'key': [], 'speechiness': [{'index': 3, 'z_score': 6.774587042246938}, {'index': 44, 'z_score': 6.703900157593376}, {'index': 143, 'z_score': 6.593942781465613}, {'index': 266, 'z_score': 2.580498552802255}, {'index': 268, 'z_score': 4.889603451485283}, {'index': 297, 'z_score': 6.515401798517211}, {'index': 325, 'z_score': 2.5647903562125745}, {'index': 336, 'z_score': 3.0753067453771896}, {'index': 404, 'z_score': 6.813857533721139}, {'index': 425, 'z_score': 4.5518772248071535}, {'index': 428, 'z_score': 3.78217559191281}, {'index': 432, 'z_score': 2.6040608476867755}, {'index': 433, 'z_score': 6.782441140541779}, {'index': 447, 'z_score': 2.7375805186990596}, {'index': 471, 'z_score': 6.099134588890679}, {'index': 515, 'z_score': 6.46042311045333}, {'index': 587, 'z_score': 2.5019575698538525}, {'index': 605, 'z_score': 2.847537894826823}, {'index': 616, 'z_score': 3.7114887072592477}, {'index': 756, 'z_score': 5.729991969033187}, {'index': 882, 'z_score': 2.509811668148693}, {'index': 888, 'z_score': 3.6722182157850476}, {'index': 943, 'z_score': 6.5075477002223705}, {'index': 984, 'z_score': 6.5075477002223705}, {'index': 1016, 'z_score': 2.957495270954586}, {'index': 1100, 'z_score': 6.578234584875933}, {'index': 1158, 'z_score': 2.6904559289300183}, {'index': 1200, 'z_score': 2.5647903562125745}, {'index': 1215, 'z_score': 4.575439519691674}, {'index': 1240, 'z_score': 6.538964093401732}, {'index': 1252, 'z_score': 6.751024747362417}, {'index': 1283, 'z_score': 6.813857533721139}, {'index': 1285, 'z_score': 6.47613130704301}, {'index': 1291, 'z_score': 4.167026408359981}, {'index': 1391, 'z_score': 6.813857533721139}, {'index': 1417, 'z_score': 6.570380486581093}, {'index': 1430, 'z_score': 2.7297264204042193}, {'index': 1547, 'z_score': 6.782441140541779}, {'index': 1583, 'z_score': 6.49183950363269}, {'index': 1635, 'z_score': 6.342611636030726}, {'index': 1683, 'z_score': 3.1774100232101126}, {'index': 1749, 'z_score': 6.633213272939814}, {'index': 1846, 'z_score': 6.326903439441045}, {'index': 1884, 'z_score': 6.82171163201598}, {'index': 1907, 'z_score': 6.711754255888216}, {'index': 1928, 'z_score': 6.664629666119175}, {'index': 1953, 'z_score': 6.14625917865972}, {'index': 1967, 'z_score': 6.452569012158489}], 'acousticness': [], 'liveness': [{'index': 2, 'z_score': 3.391313424374658}, {'index': 3, 'z_score': 3.2043342334940643}, {'index': 10, 'z_score': 3.6914642307882417}, {'index': 38, 'z_score': 3.066560092845206}, {'index': 41, 'z_score': 3.6176566554406384}, {'index': 66, 'z_score': 3.7800333212053645}, {'index': 73, 'z_score': 3.637338675533332}, {'index': 106, 'z_score': 2.864819386895093}, {'index': 136, 'z_score': 3.568451605208903}, {'index': 158, 'z_score': 2.7516477713621015}, {'index': 209, 'z_score': 3.3273468590734017}, {'index': 297, 'z_score': 3.2683007987953197}, {'index': 387, 'z_score': 3.5832131202784243}, {'index': 401, 'z_score': 3.553690090139383}, {'index': 404, 'z_score': 2.953388477312215}, {'index': 428, 'z_score': 2.8303758517328776}, {'index': 452, 'z_score': 2.845137366802399}, {'index': 453, 'z_score': 3.2289367586099313}, {'index': 466, 'z_score': 3.36671089925879}, {'index': 492, 'z_score': 3.297823828934361}, {'index': 493, 'z_score': 2.6286351457827646}, {'index': 503, 'z_score': 3.637338675533332}, {'index': 517, 'z_score': 3.2240162535867585}, {'index': 521, 'z_score': 3.7701923111590174}, {'index': 524, 'z_score': 2.7024427211303665}, {'index': 525, 'z_score': 3.6619412006492005}, {'index': 537, 'z_score': 3.1305266581464615}, {'index': 579, 'z_score': 3.2092547385172376}, {'index': 581, 'z_score': 2.569589085504682}, {'index': 588, 'z_score': 3.5241670600003423}, {'index': 606, 'z_score': 3.6028951403711185}, {'index': 696, 'z_score': 3.529087565023515}, {'index': 702, 'z_score': 3.35194938418927}, {'index': 757, 'z_score': 3.35194938418927}, {'index': 785, 'z_score': 3.504485039907648}, {'index': 835, 'z_score': 2.9238654471731738}, {'index': 859, 'z_score': 2.6089531256900704}, {'index': 861, 'z_score': 3.6816232207418946}, {'index': 864, 'z_score': 2.5400660553656413}, {'index': 871, 'z_score': 3.376551909305137}, {'index': 873, 'z_score': 3.3125853440038817}, {'index': 876, 'z_score': 3.5143260499539952}, {'index': 879, 'z_score': 3.760351301112671}, {'index': 925, 'z_score': 3.0517985777756853}, {'index': 932, 'z_score': 3.7554307960894966}, {'index': 943, 'z_score': 2.535145550342467}, {'index': 1014, 'z_score': 3.7406692810199766}, {'index': 1034, 'z_score': 3.017355042613471}, {'index': 1070, 'z_score': 3.2240162535867585}, {'index': 1100, 'z_score': 3.2535392837257997}, {'index': 1187, 'z_score': 3.1403676681928085}, {'index': 1254, 'z_score': 2.5991121156437234}, {'index': 1256, 'z_score': 3.5733721102320772}, {'index': 1285, 'z_score': 3.263380293772147}, {'index': 1287, 'z_score': 3.8046358463212315}, {'index': 1297, 'z_score': 3.415915949490525}, {'index': 1321, 'z_score': 3.7062257458577617}, {'index': 1329, 'z_score': 3.7062257458577617}, {'index': 1332, 'z_score': 3.7947948362748845}, {'index': 1337, 'z_score': 3.7357487759968024}, {'index': 1362, 'z_score': 3.5832131202784243}, {'index': 1367, 'z_score': 3.5979746353479443}, {'index': 1415, 'z_score': 3.0370370627061654}, {'index': 1418, 'z_score': 3.2240162535867585}, {'index': 1439, 'z_score': 3.6717822106955476}, {'index': 1460, 'z_score': 2.7221247412230607}, {'index': 1469, 'z_score': 3.5733721102320772}, {'index': 1540, 'z_score': 3.1157651430769415}, {'index': 1553, 'z_score': 2.544986560388814}, {'index': 1580, 'z_score': 2.6335556508059375}, {'index': 1583, 'z_score': 3.273221303818494}, {'index': 1598, 'z_score': 2.6433966608522845}, {'index': 1641, 'z_score': 3.7406692810199766}, {'index': 1661, 'z_score': 3.7062257458577617}, {'index': 1683, 'z_score': 3.3027443339575346}, {'index': 1707, 'z_score': 3.6963847358114146}, {'index': 1738, 'z_score': 2.7713297914547956}, {'index': 1749, 'z_score': 3.0616395878220324}, {'index': 1763, 'z_score': 3.5143260499539952}, {'index': 1792, 'z_score': 3.568451605208903}, {'index': 1801, 'z_score': 3.1895727184245435}, {'index': 1842, 'z_score': 3.588133625301597}, {'index': 1846, 'z_score': 2.5991121156437234}, {'index': 1884, 'z_score': 2.52530454029612}, {'index': 1907, 'z_score': 3.622577160463812}, {'index': 1923, 'z_score': 2.869739891918266}, {'index': 1927, 'z_score': 3.0419575677293382}, {'index': 1953, 'z_score': 3.1994137284708906}, {'index': 1967, 'z_score': 3.1059241330305944}, {'index': 1972, 'z_score': 3.7209872609272825}], 'instrumentalness': [], 'valence': []}\n"
     ]
    }
   ],
   "source": [
    "check_outliers_code_writer = student_a\n",
    "# 'year','danceability','energy','key','loudness','speechiness','acousticness','liveness','instrumentalness','valence','tempo',\n",
    "\n",
    "def check_outliers(data: pd.DataFrame, threshold=3.0, columns=('year','danceability','energy','key','speechiness','acousticness','liveness','instrumentalness','valence',)) -> dict:\n",
    "    results = {}\n",
    "\n",
    "    ### DIRTY HACK\n",
    "    ### REPLACE WITH YOUR ACTUAL OUTLIER CHECKING\n",
    "\n",
    "    tmp = data.copy()\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "\n",
    "    for col in columns:\n",
    "        values = tmp[col].astype(float)\n",
    "\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "\n",
    "        mask = np.abs(z_scores) > threshold\n",
    "        outliers = values[mask].index\n",
    "\n",
    "        outlier_info = [\n",
    "            {\n",
    "                'index': int(idx),\n",
    "                'z_score': float(z_scores.loc[idx])\n",
    "            }\n",
    "            for idx in outliers\n",
    "        ]\n",
    "\n",
    "        if len(outlier_info) > 0:\n",
    "            unique_count = tmp[col].nunique(dropna=True)\n",
    "            print(f\"{col}: {unique_count} unique values (column had outliers)\")\n",
    "\n",
    "        results[col] = outlier_info\n",
    "\n",
    "    # ---- NEW PART: total unique outlier indices across all columns ----\n",
    "    all_outlier_indices = {entry['index'] for col in results for entry in results[col]}\n",
    "    print(f\"Total unique outlier IDs across ALL columns: {len(all_outlier_indices)}\")\n",
    "    return results\n",
    "\n",
    "start_time_co = now()\n",
    "outliers_report = check_outliers(data, threshold=2.5)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(outliers_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => in this case a report\n",
    "# 2. activity inspects the outcome and derives decisions => in this case to remove the outliers that were found\n",
    "# 3. activity follows up on the decision by changing the data => will be done in the data preparation phase\n",
    "\n",
    "# 1. Activty: Checking for outliers and creating the report\n",
    "co_ass_uuid_executor = \"15085e9d-15f1-4727-9b6e-776dd07fcd08\"\n",
    "check_outliers_executor = [\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "# engine.insert(check_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"cd4970df-9f40-4bb1-8fad-e4dc4fcdd284\"\n",
    "co_comment = \"\"\"\n",
    "Identifying outliers with a dirty hack that uses the z-score of each row within in column and reports all values \n",
    "with a z-score higher than 2.2 as an outlier, which is not a reasonable threshold but used here to avoid not \n",
    "finding any outliers for demonstration purposes.\n",
    "\"\"\"\n",
    "check_outliers_activity = [\n",
    "    ':check_outliers rdf:type prov:Activity .',\n",
    "    ':check_outliers sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_outliers rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':check_outliers rdfs:comment \"\"\"{co_comment}\"\"\" .', \n",
    "    f':check_outliers prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{check_outliers_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_outliers prov:used :data .',\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{json.dumps(outliers_report, indent=2)}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :check_outliers .',\n",
    "    # ...\n",
    "]\n",
    "# engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"6eaa2c0a-e592-4d85-b37f-d695844910cf\"\n",
    "ior_comment = \"\"\"\n",
    "After inspecting the report the decision has been made to remove all outliers that were identfied for demonstration purpose\n",
    "\"\"\"\n",
    "inspect_outlier_report_executor = student_a\n",
    "inspect_outlier_report_activity = [\n",
    "    ':inspect_outlier_report rdf:type prov:Activity .',\n",
    "    ':inspect_outlier_report rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_outlier_report rdfs:comment \"\"\"{co_comment}\"\"\" .', \n",
    "    f':inspect_outlier_report prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_outlier_report_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_outlier_report prov:used :outlier_report .',\n",
    "    ':outlier_decision rdf:type prov:Entity .',\n",
    "    f':outlier_decision rdfs:comment \"\"\"Removing all outliers for demonstration purposes.\"\"\" .',\n",
    "    ':outlier_decision prov:wasGeneratedBy :inspect_outlier_report .',\n",
    "    # ...\n",
    "]\n",
    "# engine.insert(inspect_outlier_report_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94dfb7-328c-432b-b7e7-1f66f03eabca",
   "metadata": {},
   "source": [
    "**Continue with other tasks of the Data Understanding phase such as checking the distribution, skewness, plausibility of values, etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b4793-5fad-4c9a-89dd-abd662f916b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781e016-c770-43d2-871a-f4f4ab7378b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "def handle_outliers(df:pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    # REMOVE OUTLIERS\n",
    "    return df\n",
    "\n",
    "start_time_td = now()\n",
    "handle_outliers(data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535488\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a137\"\n",
    "td_comment = \"\"\"\n",
    "Removing all outliers that were identifying in the Data Understanding Phase.\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cff7-8fd5-4ba1-8913-b4f1ccdfda35",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8800ce26b8f3e2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20b8e8-7d7f-4df5-ba38-62704f020c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447e864-ca19-41de-b61a-e2e73863ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data prov:wasDerivedFrom :cleaned_data .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    # ....\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "    # you can also define your Evaluation Measures here\n",
    "    \n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    \n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .', \n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',     \n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    \n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now() \n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "\n",
    "    # Predict and evaluation on test data\n",
    "        \n",
    "    return 'Performance'\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now() \n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\" # Generate once\n",
    "final_model = \":final_model_entity\" \n",
    "test_set = \":test_set\" \n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    " \n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "    \n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This cell includes exemplary queries for different phases\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m### Author Block\u001b[39;00m\n\u001b[32m      5\u001b[39m author_query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mprefix_header\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33mPREFIX iao: <http://purl.obolibrary.org/obo/>\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m res_authors = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m author_block_latex = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res_authors.empty: \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/starvers/starvers.py:413\u001b[39m, in \u001b[36mTripleStoreEngine.query\u001b[39m\u001b[34m(self, select_statement, timestamp, yn_timestamp_query, as_df)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m#self.sparql_get_with_post.queryType = 'SELECT'\u001b[39;00m\n\u001b[32m    412\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRetrieving results ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_get_with_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe result has the return type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult._get_responseFormat()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m as_df:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/BI2025/lib/python3.11/urllib/request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {} # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives  = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'author_block_latex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m latex_content = \u001b[33mrf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdocumentclass[sigconf]\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33macmart\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mAtBeginDocument\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mprovidecommand\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mBibTeX\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m Bib\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mTeX \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msetcopyright\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33macmlicensed\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mcopyrightyear\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m2025\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33m\\\u001b[39m\u001b[33macmYear\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m2025\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33m\\\u001b[39m\u001b[33macmDOI\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mXXXXXXX.XXXXXXX\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33m\\\u001b[39m\u001b[33macmConference[BI 2025]\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mBusiness Intelligence\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mdocument\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mtitle\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mBI2025 Experiment Report - Group \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33m%% ---Authors: Dynamically added ---\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mauthor_block_latex\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mabstract\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33m  This report documents the machine learning experiment for Group \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, following the CRISP-DM process model.\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mabstract\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mccsdesc[500]\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mComputing methodologies~Machine learning\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mkeywords\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mCRISP-DM, Provenance, Knowledge Graph, Machine Learning\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mmaketitle\u001b[39m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[33m%% --- 1. Business Understanding ---\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mBusiness Understanding\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msubsection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mData Source and Scenario\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mbu_data_source\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msubsection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mBusiness Objectives\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mbu_objectives\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[33m%% --- 2. Data Understanding ---\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mData Understanding\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mDataset Description:\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdu_description\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33mThe following features were identified in the dataset:\u001b[39m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtable\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m[h]\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcaption\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mRaw Data Features\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlabel\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtab:features\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtabular\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mlp\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m0.2\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlinewidth\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33mp\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m0.4\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlinewidth\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtoprule\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mFeature Name\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m & \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mData Type\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m & \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mDescription\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\\\u001b[39m\u001b[33m\\\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmidrule\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[33m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdu_table_rows\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbottomrule\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtabular\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtable\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     52\u001b[39m \n\u001b[32m     53\u001b[39m \u001b[33m%% --- 3. Data Preparation ---\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mData Preparation\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msubsection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mData Cleaning\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33mDescribe your Data preparation steps here and include respective graph data.\u001b[39m\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[33m%% --- 4. Modeling ---\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mModeling\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     61\u001b[39m \n\u001b[32m     62\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msubsection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mHyperparameter Configuration\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33mThe model was trained using the following hyperparameter settings:\u001b[39m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtable\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m[h]\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcaption\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mHyperparameter Settings\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlabel\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtab:hyperparams\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtabular\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mlp\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33m0.4\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlinewidth\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33ml\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtoprule\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mParameter\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m & \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mDescription\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m & \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mValue\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\\\u001b[39m\u001b[33m\\\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmidrule\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhp_table_rows\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbottomrule\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m  \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtabular\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mtable\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msubsection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mTraining Run\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33mA training run was executed with the following characteristics:\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mbegin\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mitemize\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mitem \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mAlgorithm:\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_algo\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mitem \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mStart Time:\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_start\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mitem \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mEnd Time:\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_end\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mitem \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mResult:\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_m_lbl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_m_val\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mitemize\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m     86\u001b[39m \u001b[33m%% --- 5. Evaluation ---\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mEvaluation\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[33m%% --- 6. Deployment ---\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mDeployment\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[33m\\\u001b[39m\u001b[33msection\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mConclusion\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     93\u001b[39m \n\u001b[32m     94\u001b[39m \u001b[33m\\\u001b[39m\u001b[33mend\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mdocument\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'author_block_latex' is not defined"
     ]
    }
   ],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "Describe your Data preparation steps here and include respective graph data.\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report written to: data/report/experiment_report.tex\n"
     ]
    }
   ],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BI2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
